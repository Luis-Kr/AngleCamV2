{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eac4c85",
   "metadata": {},
   "source": [
    "### AngleCam V2 TLS reference testing setup\n",
    "\n",
    "This notebook initializes the environment to test AngleCam V2 outputs against TLS references. Below, we import required packages, and load the pretrained model checkpoint for subsequent evaluation steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56636f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and path setup\n",
    "from __future__ import annotations\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Union, List, Dict, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "from datetime import datetime\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Ensure project root is on sys.path for imports if needed\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent.parent.parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))\n",
    "    \n",
    "# AngleCam import\n",
    "from anglecam import AngleCam\n",
    "\n",
    "# Paths\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"data\" / \"03_Model_Outputs\" / \"checkpoint\"\n",
    "CHECKPOINT_PATH = OUTPUT_DIR / \"AngleCamV2.pth\"\n",
    "\n",
    "TESTING_CSV = PROJECT_ROOT / \"data\" / \"01_Training_Validation_Data\" / \"splits\" / \"testing.csv\"\n",
    "IMAGES_DIR = PROJECT_ROOT / \"data\" / \"01_Training_Validation_Data\" / \"image_data\"\n",
    "\n",
    "TLS_DIR = PROJECT_ROOT / \"data\" / \"02_Test_Data\" / \"TLS\" / \"analysis\"\n",
    "\n",
    "TESTING_DIR_CALATHEA = PROJECT_ROOT / \"data\" / \"02_Test_Data\" / \"images\" / \"calathea_ornata\"\n",
    "TESTING_DIR_MARANTA = PROJECT_ROOT / \"data\" / \"02_Test_Data\" / \"images\" / \"maranta_leuconeura\"\n",
    "TESTING_DIR_AGLAONEMA = PROJECT_ROOT / \"data\" / \"02_Test_Data\" / \"images\" / \"aglaonema_commutatum\"\n",
    "\n",
    "TESTING_RESULTS_DIR = PROJECT_ROOT / \"data\" / \"03_Model_Outputs\" / \"predictions\" / \"testing\"\n",
    "FIGURE_OUTPUT_DIR = PROJECT_ROOT / \"data\" / \"other\" / \"figures\" / \"results\"\n",
    "\n",
    "# Colors\n",
    "custom_colors = [\"#C2B2B4\", \"#6B4E71\", \"#3A4454\", \"#53687E\", \"#F5DDDD\"]\n",
    "custom_colors2 = [\"#F1F1FE\", \"#9492B9\", \"#AFBFCD\", \"#3A739D\", \"#ADC0A8\"]\n",
    "\n",
    "# Create color palette\n",
    "custom_palette = sns.color_palette(custom_colors)\n",
    "custom_palette2 = sns.color_palette(custom_colors2)\n",
    "\n",
    "print(f\"Using checkpoint: {CHECKPOINT_PATH}\")\n",
    "print(f\"Testing CSV:      {TESTING_CSV}\")\n",
    "print(f\"Images dir:       {IMAGES_DIR}\")\n",
    "\n",
    "# Basic checks\n",
    "assert CHECKPOINT_PATH.exists(), f\"Checkpoint not found: {CHECKPOINT_PATH}\"\n",
    "assert TESTING_CSV.exists(), f\"Testing CSV not found: {TESTING_CSV}\"\n",
    "assert IMAGES_DIR.exists(), f\"Images directory not found: {IMAGES_DIR}\"\n",
    "assert TLS_DIR.exists(), f\"TLS directory not found: {TLS_DIR}\"\n",
    "assert TESTING_DIR_CALATHEA.exists(), f\"Calathea directory not found: {TESTING_DIR_CALATHEA}\"\n",
    "assert TESTING_DIR_MARANTA.exists(), f\"Maranta directory not found: {TESTING_DIR_MARANTA}\"\n",
    "assert TESTING_DIR_AGLAONEMA.exists(), f\"Aglaonema directory not found: {TESTING_DIR_AGLAONEMA}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26de022a",
   "metadata": {},
   "source": [
    "### Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2480f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "config_overrides = {\n",
    "    \"training\": {\n",
    "        \"output_dir\": \"data/model/output\" \n",
    "    }\n",
    "}\n",
    "\n",
    "anglecam = AngleCam.from_checkpoint(\n",
    "    str(CHECKPOINT_PATH), \n",
    "    config_overrides=config_overrides\n",
    ")\n",
    "anglecam.device = device\n",
    "\n",
    "# Move model to selected device for later inference\n",
    "anglecam.model.to(device)\n",
    "\n",
    "print(\"Model loaded and ready for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1712a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the model is working\n",
    "anglecam.predict(IMAGES_DIR / \"01_1_TLC00006_image_000003.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a653250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TLS_DIR / \"calathea_tls_leaf_angle_analysis.json\", \"r\") as f:\n",
    "    calathea_tls_data = json.load(f)\n",
    "\n",
    "with open(TLS_DIR / \"maranta_tls_leaf_angle_analysis.json\", \"r\") as f:\n",
    "    maranta_tls_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c1a30e",
   "metadata": {},
   "source": [
    "### Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4753e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load testing csv file\n",
    "testing_df = pd.read_csv(TESTING_CSV)\n",
    "\n",
    "calathea_df = testing_df[testing_df['plant_name'] == 'Calathea ornata']\n",
    "maranta_df = testing_df[testing_df['plant_name'] == 'Maranta leuconeura']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6fb42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "calathea_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_tls_reference(df, anglecam, images_dir, plant_name):\n",
    "    \"\"\"\n",
    "    Perform predictions and compare with TLS reference data.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with test data (columns: datetime, filename, las_file, plant_name, tls_mean_angle)\n",
    "        anglecam: Loaded AngleCam model instance\n",
    "        images_dir: Path to directory containing the images\n",
    "        plant_name: Name of the plant species\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains prediction results, reference data, and comparison metrics\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        \"predictions\": [],\n",
    "        \"summary\": {},\n",
    "        \"metadata\": {\n",
    "            \"total_samples\": len(df),\n",
    "            \"plant_species\": plant_name,\n",
    "            \"prediction_timestamp\": pd.Timestamp.now().isoformat(),\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Collect data for batch processing\n",
    "    image_paths = []\n",
    "    reference_means = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        image_path = images_dir / row[\"filename\"]\n",
    "        if image_path.exists():\n",
    "            image_paths.append(str(image_path))\n",
    "            reference_means.append(row[\"tls_average_angle\"])\n",
    "        else:\n",
    "            print(f\"Warning: Image not found: {image_path}\")\n",
    "\n",
    "    print(f\"Processing {len(image_paths)} images...\")\n",
    "\n",
    "    # Perform batch prediction with reference values\n",
    "    prediction_results = anglecam.predict(image_paths, reference_mean=reference_means)\n",
    "\n",
    "    # Process results\n",
    "    predicted_angles = []\n",
    "    reference_angles = []\n",
    "    errors = []\n",
    "    absolute_errors = []\n",
    "\n",
    "    for i, pred_result in enumerate(prediction_results):\n",
    "        if \"error\" not in pred_result:\n",
    "            # Get corresponding row data\n",
    "            row = df.iloc[i]\n",
    "\n",
    "            # Store individual result\n",
    "            result_entry = {\n",
    "                \"datetime\": row[\"time\"],\n",
    "                \"filename\": row[\"filename\"],\n",
    "                \"las_file\": row[\"tls_filename\"],\n",
    "                \"predicted_mean_angle\": pred_result[\"predicted_mean_leaf_angle\"],\n",
    "                \"reference_mean_angle\": pred_result[\"reference_mean_leaf_angle\"],\n",
    "                \"prediction_error\": pred_result[\"prediction_error\"],\n",
    "                \"absolute_error\": pred_result[\"absolute_error\"],\n",
    "                \"angle_distribution\": pred_result[\"angle_distribution\"],\n",
    "                \"image_path\": pred_result[\"image_path\"],\n",
    "            }\n",
    "            results[\"predictions\"].append(result_entry)\n",
    "\n",
    "            # Collect for summary statistics\n",
    "            predicted_angles.append(pred_result[\"predicted_mean_leaf_angle\"])\n",
    "            reference_angles.append(pred_result[\"reference_mean_leaf_angle\"])\n",
    "            errors.append(pred_result[\"prediction_error\"])\n",
    "            absolute_errors.append(pred_result[\"absolute_error\"])\n",
    "        else:\n",
    "            print(\n",
    "                f\"Prediction error for {pred_result['image_path']}: {pred_result['error']}\"\n",
    "            )\n",
    "\n",
    "    # Calculate summary statistics\n",
    "    if predicted_angles:\n",
    "        predicted_angles = np.array(predicted_angles)\n",
    "        reference_angles = np.array(reference_angles)\n",
    "        errors = np.array(errors)\n",
    "        absolute_errors = np.array(absolute_errors)\n",
    "\n",
    "        # Basic statistics\n",
    "        results[\"summary\"] = {\n",
    "            \"n_successful_predictions\": len(predicted_angles),\n",
    "            \"n_failed_predictions\": len(prediction_results) - len(predicted_angles),\n",
    "            # Prediction statistics\n",
    "            \"predicted_mean\": float(np.mean(predicted_angles)),\n",
    "            \"predicted_std\": float(np.std(predicted_angles)),\n",
    "            \"predicted_min\": float(np.min(predicted_angles)),\n",
    "            \"predicted_max\": float(np.max(predicted_angles)),\n",
    "            # Reference statistics\n",
    "            \"reference_mean\": float(np.mean(reference_angles)),\n",
    "            \"reference_std\": float(np.std(reference_angles)),\n",
    "            \"reference_min\": float(np.min(reference_angles)),\n",
    "            \"reference_max\": float(np.max(reference_angles)),\n",
    "            # Error statistics\n",
    "            \"mean_error\": float(np.mean(errors)),\n",
    "            \"std_error\": float(np.std(errors)),\n",
    "            \"mean_absolute_error\": float(np.mean(absolute_errors)),\n",
    "            \"rmse\": float(np.sqrt(np.mean(errors**2))),\n",
    "            # Correlation and agreement\n",
    "            \"correlation\": float(np.corrcoef(predicted_angles, reference_angles)[0, 1]),\n",
    "            \"r_squared\": float(\n",
    "                np.corrcoef(predicted_angles, reference_angles)[0, 1] ** 2\n",
    "            ),\n",
    "            # Angle range statistics\n",
    "            \"predicted_range\": float(\n",
    "                np.max(predicted_angles) - np.min(predicted_angles)\n",
    "            ),\n",
    "            \"reference_range\": float(\n",
    "                np.max(reference_angles) - np.min(reference_angles)\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        print(f\"\\n=== Prediction Summary ===\")\n",
    "        print(\n",
    "            f\"Successful predictions: {results['summary']['n_successful_predictions']}\"\n",
    "        )\n",
    "        print(f\"Mean Absolute Error: {results['summary']['mean_absolute_error']:.2f}°\")\n",
    "        print(f\"RMSE: {results['summary']['rmse']:.2f}°\")\n",
    "        print(f\"Correlation (r): {results['summary']['correlation']:.3f}\")\n",
    "        print(f\"R²: {results['summary']['r_squared']:.3f}\")\n",
    "        print(\n",
    "            f\"Predicted range: {results['summary']['predicted_min']:.1f}° - {results['summary']['predicted_max']:.1f}°\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Reference range: {results['summary']['reference_min']:.1f}° - {results['summary']['reference_max']:.1f}°\"\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def predict_directory_images_fast(\n",
    "    images_dir: Union[str, Path], \n",
    "    anglecam, \n",
    "    plant_name: Optional[str] = None,\n",
    "    batch_size: int = 64\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fast batch processing that truly processes images in parallel.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from PIL import Image\n",
    "    import time\n",
    "    from pathlib import Path\n",
    "    \n",
    "    images_dir = Path(images_dir)\n",
    "    \n",
    "    if not images_dir.exists():\n",
    "        raise ValueError(f\"Directory not found: {images_dir}\")\n",
    "    \n",
    "    # Find all images\n",
    "    image_extensions = ['*.png', '*.jpg', '*.jpeg', '*.PNG', '*.JPG', '*.JPEG']\n",
    "    image_paths = []\n",
    "    for ext in image_extensions:\n",
    "        image_paths.extend(list(images_dir.glob(ext)))\n",
    "    \n",
    "    if not image_paths:\n",
    "        raise ValueError(f\"No supported image files found in directory: {images_dir}\")\n",
    "    \n",
    "    image_paths = sorted(image_paths)\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} images\")\n",
    "    print(f\"Processing with true batch size {batch_size}...\")\n",
    "    \n",
    "    # Ensure predictor exists to get transform\n",
    "    if anglecam.predictor is None:\n",
    "        from anglecam.inference.predictor import AngleCamPredictor\n",
    "        anglecam.predictor = AngleCamPredictor(\n",
    "            model=anglecam.model,\n",
    "            config=anglecam.config,\n",
    "            device=anglecam.device,\n",
    "            logger=anglecam.logger,\n",
    "        )\n",
    "    \n",
    "    # Get transform\n",
    "    transform = anglecam.predictor.transform\n",
    "    \n",
    "    # Results storage\n",
    "    results = {\n",
    "        \"predictions\": [],\n",
    "        \"summary\": {},\n",
    "        \"metadata\": {\n",
    "            \"total_images_found\": len(image_paths),\n",
    "            \"images_directory\": str(images_dir),\n",
    "            \"plant_species\": plant_name,\n",
    "            \"prediction_timestamp\": pd.Timestamp.now().isoformat(),\n",
    "            \"batch_size\": batch_size\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    predicted_angles = []\n",
    "    successful_predictions = 0\n",
    "    failed_predictions = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Set model to eval mode\n",
    "    anglecam.model.eval()\n",
    "    \n",
    "    # Process in true batches\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i + batch_size]\n",
    "            \n",
    "            # Load and preprocess batch of images\n",
    "            batch_tensors = []\n",
    "            valid_paths = []\n",
    "            \n",
    "            for path in batch_paths:\n",
    "                try:\n",
    "                    # Load image\n",
    "                    image = Image.open(path).convert(\"RGB\")\n",
    "                    \n",
    "                    # Apply transform\n",
    "                    if hasattr(transform, \"__call__\"):\n",
    "                        # Albumentations\n",
    "                        transformed = transform(image=np.array(image))\n",
    "                        tensor = transformed[\"image\"]\n",
    "                    else:\n",
    "                        # Torchvision\n",
    "                        tensor = transform(image)\n",
    "                    \n",
    "                    batch_tensors.append(tensor)\n",
    "                    valid_paths.append(path)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to load {path.name}: {e}\")\n",
    "                    failed_predictions += 1\n",
    "            \n",
    "            if not batch_tensors:\n",
    "                continue\n",
    "                \n",
    "            # Stack into batch tensor\n",
    "            batch_tensor = torch.stack(batch_tensors).to(anglecam.device)\n",
    "            \n",
    "            try:\n",
    "                # Single forward pass for entire batch\n",
    "                outputs = anglecam.model(batch_tensor)\n",
    "                probabilities_batch = outputs.cpu().numpy()\n",
    "                \n",
    "                # Process results\n",
    "                for j, (probabilities, path) in enumerate(zip(probabilities_batch, valid_paths)):\n",
    "                    try:\n",
    "                        # Calculate angle statistics\n",
    "                        angle_stats = anglecam.predictor._probabilities_to_stats(probabilities)\n",
    "                        \n",
    "                        result_entry = {\n",
    "                            \"filename\": path.name,\n",
    "                            \"image_path\": str(path),\n",
    "                            \"predicted_mean_angle\": angle_stats[\"mean_angle\"],\n",
    "                            \"angle_distribution\": probabilities.tolist(),\n",
    "                        }\n",
    "                        results[\"predictions\"].append(result_entry)\n",
    "                        predicted_angles.append(angle_stats[\"mean_angle\"])\n",
    "                        successful_predictions += 1\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing result for {path.name}: {e}\")\n",
    "                        failed_predictions += 1\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Batch inference failed: {e}\")\n",
    "                failed_predictions += len(batch_tensors)\n",
    "            \n",
    "            # Progress update\n",
    "            elapsed = time.time() - start_time\n",
    "            processed = min(i + batch_size, len(image_paths))\n",
    "            rate = processed / elapsed if elapsed > 0 else 0\n",
    "            eta = (len(image_paths) - processed) / rate if rate > 0 else 0\n",
    "            \n",
    "            print(f\"Processed {processed}/{len(image_paths)} images \"\n",
    "                  f\"({rate:.1f} img/s, ETA: {eta:.1f}s)\")\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    if predicted_angles:\n",
    "        predicted_angles = np.array(predicted_angles)\n",
    "        \n",
    "        results[\"summary\"] = {\n",
    "            \"n_successful_predictions\": successful_predictions,\n",
    "            \"n_failed_predictions\": failed_predictions,\n",
    "            \"success_rate\": successful_predictions / len(image_paths),\n",
    "            \"processing_time_seconds\": processing_time,\n",
    "            \"images_per_second\": len(image_paths) / processing_time,\n",
    "            \n",
    "            \"predicted_mean\": float(np.mean(predicted_angles)),\n",
    "            \"predicted_std\": float(np.std(predicted_angles)),\n",
    "            \"predicted_min\": float(np.min(predicted_angles)),\n",
    "            \"predicted_max\": float(np.max(predicted_angles)),\n",
    "            \"predicted_median\": float(np.median(predicted_angles)),\n",
    "            \"predicted_range\": float(np.max(predicted_angles) - np.min(predicted_angles)),\n",
    "            \"predicted_25th_percentile\": float(np.percentile(predicted_angles, 25)),\n",
    "            \"predicted_75th_percentile\": float(np.percentile(predicted_angles, 75)),\n",
    "        }\n",
    "        \n",
    "        print(f\"Total time: {processing_time:.2f}s\")\n",
    "        print(f\"Speed: {results['summary']['images_per_second']:.1f} images/second\")\n",
    "        print(f\"Successful: {successful_predictions}/{len(image_paths)} ({results['summary']['success_rate']:.1%})\")\n",
    "        print(f\"Mean angle: {results['summary']['predicted_mean']:.2f}°\")\n",
    "        \n",
    "    else:\n",
    "        results[\"summary\"] = {\n",
    "            \"n_successful_predictions\": 0,\n",
    "            \"n_failed_predictions\": failed_predictions,\n",
    "            \"success_rate\": 0.0,\n",
    "            \"processing_time_seconds\": processing_time,\n",
    "            \"images_per_second\": 0.0\n",
    "        }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae211f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calathea ornata testing inference\n",
    "calathea_results = predict_with_tls_reference(calathea_df, anglecam, TESTING_DIR_CALATHEA, \"Calathea ornata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e855783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maranta leuconeura testing inference\n",
    "maranta_results = predict_with_tls_reference(maranta_df, anglecam, TESTING_DIR_MARANTA, \"Maranta leuconeura\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6288b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "aglaonema_results = predict_directory_images_fast(\n",
    "    images_dir=TESTING_DIR_AGLAONEMA,\n",
    "    anglecam=anglecam,\n",
    "    plant_name=\"Aglaonema commutatum\",\n",
    "    batch_size=128 # Start with 64, increase if GPU memory allows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcca2afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save json file\n",
    "def save_prediction_results(\n",
    "    results_dict: dict, species_name: str, output_dir: str = \"data/results/predictions\"\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Save prediction results to JSON file.\n",
    "\n",
    "    Args:\n",
    "        results_dict: Results dictionary from predict_directory_images\n",
    "        species_name: Name of the plant species\n",
    "        output_dir: Directory to save the file\n",
    "\n",
    "    Returns:\n",
    "        Path to saved file\n",
    "    \"\"\"\n",
    "    import json\n",
    "    from pathlib import Path\n",
    "\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Validate results structure\n",
    "    required_keys = [\"predictions\", \"summary\", \"metadata\"]\n",
    "    if not all(key in results_dict for key in required_keys):\n",
    "        raise ValueError(f\"Results dict missing required keys: {required_keys}\")\n",
    "\n",
    "    # Create filename\n",
    "    filename = f\"{species_name.lower().replace(' ', '_')}_predictions.json\"\n",
    "    filepath = output_path / filename\n",
    "\n",
    "    # Save with pretty formatting\n",
    "    with open(filepath, \"w\") as f:\n",
    "        json.dump(results_dict, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\n",
    "        f\"Saved {len(results_dict['predictions'])} predictions for {species_name} to {filepath}\"\n",
    "    )\n",
    "\n",
    "    return filepath\n",
    "\n",
    "\n",
    "# Usage:\n",
    "#calathea_file = save_prediction_results(calathea_results, \"Calathea ornata\", TESTING_RESULTS_DIR)\n",
    "maranta_file = save_prediction_results(maranta_results, \"Maranta leuconeura\", TESTING_RESULTS_DIR)\n",
    "aglaonema_file = save_prediction_results(aglaonema_results, \"Aglaonema commutatum\", TESTING_RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0071e34",
   "metadata": {},
   "source": [
    "### Load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8165c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TESTING_RESULTS_DIR / \"calathea_ornata_predictions.json\", \"r\") as f:\n",
    "    calathea_results = json.load(f)\n",
    "\n",
    "with open(TESTING_RESULTS_DIR / \"maranta_leuconeura_predictions.json\", \"r\") as f:\n",
    "    maranta_results = json.load(f)\n",
    "\n",
    "with open(TESTING_RESULTS_DIR / \"aglaonema_commutatum_predictions.json\", \"r\") as f:\n",
    "    aglaonema_results = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4002cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_plot_data(results):\n",
    "    \"\"\"\n",
    "    Extract plotting data from results list.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for result in results['predictions']: \n",
    "        data.append({\n",
    "            'filename': result['filename'],\n",
    "            'predicted_mean_angle': result['predicted_mean_angle'],\n",
    "            'reference_mean_angle': result['reference_mean_angle'],\n",
    "            'timestamp': datetime.strptime(result['datetime'], \"%d-%m-%Y %H:%M\")\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0feb3260",
   "metadata": {},
   "source": [
    "### Testing result calathea ornata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9cf41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced plotting function with day/night visualization\n",
    "plot_df = prepare_plot_data(calathea_results)\n",
    "\n",
    "# Set all plotting parameters at the beginning for clarity and reproducibility\n",
    "FIGURE_SIZE = (22, 6)\n",
    "ROLLING_WINDOW = 3\n",
    "MARKER_SIZE = 80\n",
    "LINE_WIDTH = 5\n",
    "ALPHA_SCATTER = 0.7\n",
    "ALPHA_GRID = 0.2\n",
    "ROTATION_ANGLE = 0\n",
    "\n",
    "# Font sizes\n",
    "XLABEL_FONTSIZE = 18\n",
    "YLABEL_FONTSIZE = 18\n",
    "LEGEND_FONTSIZE = 18\n",
    "TICK_LABELSIZE = 18\n",
    "\n",
    "# Colors for time series\n",
    "ANGLECAM_COLOR = custom_palette2[3]\n",
    "ANGLECAM_LINE_COLOR = custom_palette2[3]\n",
    "TLS_COLOR = custom_palette[1]\n",
    "TLS_LINE_COLOR = custom_palette[1]\n",
    "\n",
    "# Day/Night visualization parameters\n",
    "NIGHT_COLOR = '#f0f0f0'  # Light grey for night background\n",
    "NIGHT_ALPHA = 1.0\n",
    "DAY_START_HOUR = 8  # 08:00 - when RGB images typically show daylight effects\n",
    "DAY_END_HOUR = 18   # 18:00 - when night period begins\n",
    "\n",
    "# Time axis formatting\n",
    "HOUR_INTERVALS = [0, 6, 12, 18]\n",
    "TIME_FORMAT = \"%H:%M\"\n",
    "\n",
    "# Scatter plot parameters\n",
    "SCATTER_MARKER_SIZE = 80\n",
    "SCATTER_ALPHA = 0.7\n",
    "ALPHA_REF_LINE = 0.99\n",
    "ALPHA_FIT_LINE = 0.9\n",
    "EDGE_LINE_WIDTH = 0.5\n",
    "REF_LINE_WIDTH = 3\n",
    "FIT_LINE_WIDTH = 3.5\n",
    "\n",
    "# Scatter plot colors\n",
    "SCATTER_COLOR = \"#595959\"\n",
    "REF_LINE_COLOR = \"#a5a5a5\"\n",
    "FIT_LINE_COLOR = \"black\"\n",
    "\n",
    "# Statistics text box positioning\n",
    "STATS_X_POS = 0.45\n",
    "STATS_Y_POS = 0.2\n",
    "\n",
    "# Line styles\n",
    "REF_LINE_STYLE = \"--\"\n",
    "FIT_LINE_STYLE = \"--\"\n",
    "\n",
    "# Tick spacing\n",
    "TICK_SPACING = 5\n",
    "\n",
    "# Regression line points for smooth consecutive line\n",
    "REGRESSION_POINTS = 100\n",
    "\n",
    "# Plot spacing and width ratios\n",
    "WIDTH_RATIOS = [2, 1]  # Time series : Scatter plot ratio\n",
    "SUBPLOT_SPACING = 0.001  # Distance between plots\n",
    "\n",
    "# Create figure with 1 row, 2 columns with custom width ratios and spacing\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    1,\n",
    "    2,\n",
    "    figsize=FIGURE_SIZE,\n",
    "    gridspec_kw={\"width_ratios\": WIDTH_RATIOS, \"wspace\": SUBPLOT_SPACING},\n",
    ")\n",
    "\n",
    "# LEFT PLOT: Time series with day/night background\n",
    "time_data = plot_df.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\")\n",
    "\n",
    "# Add day/night background shading\n",
    "time_min = time_data[\"timestamp\"].min()\n",
    "time_max = time_data[\"timestamp\"].max()\n",
    "\n",
    "# Create night period spans for each day in the dataset\n",
    "import pandas as pd\n",
    "from datetime import datetime, time\n",
    "\n",
    "# Get all unique dates in the dataset\n",
    "dates = pd.to_datetime(time_data[\"timestamp\"]).dt.date.unique()\n",
    "\n",
    "for date in dates:\n",
    "    # Convert date to datetime for the start of the day\n",
    "    day_start = pd.Timestamp.combine(date, time(0, 0))\n",
    "    day_end = pd.Timestamp.combine(date, time(23, 59, 59))\n",
    "    \n",
    "    # Define night periods: 00:00-08:00 and 18:00-24:00\n",
    "    night_start_early = day_start\n",
    "    night_end_early = pd.Timestamp.combine(date, time(DAY_START_HOUR, 0))\n",
    "    \n",
    "    night_start_late = pd.Timestamp.combine(date, time(DAY_END_HOUR, 0))\n",
    "    night_end_late = day_end\n",
    "    \n",
    "    # Only shade if the night period overlaps with our data range\n",
    "    if night_end_early >= time_min and night_start_early <= time_max:\n",
    "        ax1.axvspan(\n",
    "            max(night_start_early, time_min), \n",
    "            min(night_end_early, time_max),\n",
    "            color=NIGHT_COLOR, \n",
    "            alpha=NIGHT_ALPHA, \n",
    "            zorder=0\n",
    "        )\n",
    "    \n",
    "    if night_end_late >= time_min and night_start_late <= time_max:\n",
    "        ax1.axvspan(\n",
    "            max(night_start_late, time_min), \n",
    "            min(night_end_late, time_max),\n",
    "            color=NIGHT_COLOR, \n",
    "            alpha=NIGHT_ALPHA, \n",
    "            zorder=0\n",
    "        )\n",
    "\n",
    "# Plot TLS standard angles\n",
    "sns.scatterplot(\n",
    "    data=time_data,\n",
    "    x=\"timestamp\",\n",
    "    y=\"reference_mean_angle\",\n",
    "    marker=\"o\",\n",
    "    color=TLS_COLOR,\n",
    "    label=\"TLS\",\n",
    "    alpha=ALPHA_SCATTER,\n",
    "    s=MARKER_SIZE,\n",
    "    ax=ax1,\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "# Add smoothed TLS moving average\n",
    "time_data[\"reference_mean_angle_smooth\"] = (\n",
    "    time_data[\"reference_mean_angle\"]\n",
    "    .rolling(window=ROLLING_WINDOW, center=True, min_periods=1)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# Plot the smoothed TLS moving average\n",
    "sns.lineplot(\n",
    "    data=time_data,\n",
    "    x=\"timestamp\",\n",
    "    y=\"reference_mean_angle_smooth\",\n",
    "    color=TLS_LINE_COLOR,\n",
    "    linewidth=LINE_WIDTH,\n",
    "    alpha=0.8,\n",
    "    ax=ax1,\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "# Plot predicted angles\n",
    "sns.scatterplot(\n",
    "    data=time_data,\n",
    "    x=\"timestamp\",\n",
    "    y=\"predicted_mean_angle\",\n",
    "    marker=\"o\",\n",
    "    color=ANGLECAM_COLOR,\n",
    "    label=\"AngleCam\",\n",
    "    alpha=ALPHA_SCATTER,\n",
    "    s=MARKER_SIZE,\n",
    "    ax=ax1,\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "# Calculate smoothed predicted mean angle\n",
    "time_data[\"predicted_mean_angle_smooth\"] = (\n",
    "    time_data[\"predicted_mean_angle\"]\n",
    "    .rolling(window=ROLLING_WINDOW, center=True, min_periods=1)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "if len(time_data) > 0:\n",
    "    time_data.iloc[0, time_data.columns.get_loc(\"predicted_mean_angle_smooth\")] = (\n",
    "        time_data.iloc[0][\"predicted_mean_angle\"]\n",
    "    )\n",
    "    time_data.iloc[-1, time_data.columns.get_loc(\"predicted_mean_angle_smooth\")] = (\n",
    "        time_data.iloc[-1][\"predicted_mean_angle\"]\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    data=time_data,\n",
    "    x=\"timestamp\",\n",
    "    y=\"predicted_mean_angle_smooth\",\n",
    "    color=ANGLECAM_LINE_COLOR,\n",
    "    linewidth=LINE_WIDTH,\n",
    "    alpha=0.8,\n",
    "    ax=ax1,\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "ax1.grid(True, alpha=ALPHA_GRID)\n",
    "\n",
    "# Create custom legend that includes day/night explanation\n",
    "from matplotlib.patches import Rectangle\n",
    "legend_elements = ax1.get_legend_handles_labels()\n",
    "# Add night period indicator to legend\n",
    "night_patch = Rectangle((0, 0), 1, 1, facecolor=NIGHT_COLOR, alpha=NIGHT_ALPHA, \n",
    "                       edgecolor='grey', linewidth=0.5)\n",
    "legend_elements[0].append(night_patch)\n",
    "legend_elements[1].append('Night (18:00-08:00)')\n",
    "\n",
    "ax1.legend(\n",
    "    legend_elements[0], legend_elements[1],\n",
    "    fontsize=LEGEND_FONTSIZE, \n",
    "    frameon=False, \n",
    "    ncol=3, \n",
    "    borderaxespad=0, \n",
    "    handletextpad=0.0,\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.07)\n",
    ")\n",
    "\n",
    "# Set time formatting for x-axis\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter(TIME_FORMAT))\n",
    "ax1.xaxis.set_major_locator(mdates.HourLocator(byhour=HOUR_INTERVALS))\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=ROTATION_ANGLE)\n",
    "ax1.spines[\"top\"].set_visible(False)\n",
    "ax1.spines[\"right\"].set_visible(False)\n",
    "ax1.tick_params(axis=\"both\", which=\"major\", labelsize=TICK_LABELSIZE)\n",
    "\n",
    "ax1.set_xlabel(\"Time of Day\", fontsize=XLABEL_FONTSIZE)\n",
    "ax1.set_ylabel(\"Leaf Angle (°)\", fontsize=YLABEL_FONTSIZE)\n",
    "\n",
    "# Calculate y-axis limits for alignment\n",
    "y_min = min(\n",
    "    time_data[\"predicted_mean_angle_smooth\"].min(),\n",
    "    time_data[\"reference_mean_angle_smooth\"].min(),\n",
    ")\n",
    "y_max = max(\n",
    "    time_data[\"predicted_mean_angle_smooth\"].max(),\n",
    "    time_data[\"reference_mean_angle_smooth\"].max(),\n",
    ")\n",
    "y_margin = 0.25 * (y_max - y_min)\n",
    "y_lim_min = y_min - y_margin\n",
    "y_lim_max = y_max + y_margin\n",
    "\n",
    "ax1.set_ylim(y_lim_min + 2, y_lim_max - 2)\n",
    "\n",
    "# RIGHT PLOT: Scatter plot \n",
    "# Filter out any NaN values\n",
    "valid_data = plot_df.dropna(subset=[\"predicted_mean_angle\", \"reference_mean_angle\"])\n",
    "\n",
    "x = valid_data[\"predicted_mean_angle\"]  # AngleCam predictions on x-axis\n",
    "y = valid_data[\"reference_mean_angle\"]  # TLS on y-axis\n",
    "\n",
    "# Add 1:1 reference line\n",
    "min_val = min(x.min(), y.min())\n",
    "max_val = max(x.max(), y.max())\n",
    "ax2.plot(\n",
    "    [min_val - 1, max_val + 1],\n",
    "    [min_val - 1, max_val + 1],\n",
    "    REF_LINE_STYLE,\n",
    "    color=REF_LINE_COLOR,\n",
    "    alpha=ALPHA_REF_LINE,\n",
    "    linewidth=REF_LINE_WIDTH,\n",
    "    label=\"1:1 line\",\n",
    "    zorder=1,\n",
    ")\n",
    "\n",
    "# Create scatter plot\n",
    "ax2.scatter(\n",
    "    x,\n",
    "    y,\n",
    "    alpha=SCATTER_ALPHA,\n",
    "    s=SCATTER_MARKER_SIZE,\n",
    "    color=SCATTER_COLOR,\n",
    "    edgecolors=SCATTER_COLOR,\n",
    "    linewidth=EDGE_LINE_WIDTH,\n",
    ")\n",
    "\n",
    "# Add regression line - create consecutive x values for smooth line\n",
    "z = np.polyfit(x, y, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(x.min(), x.max(), REGRESSION_POINTS)\n",
    "ax2.plot(\n",
    "    x_line,\n",
    "    p(x_line),\n",
    "    \"-\",\n",
    "    alpha=ALPHA_FIT_LINE,\n",
    "    linewidth=FIT_LINE_WIDTH,\n",
    "    label=f\"Fit: y = {z[0]:.2f}x + {z[1]:.2f}\",\n",
    "    color=FIT_LINE_COLOR,\n",
    ")\n",
    "\n",
    "# Calculate metrics\n",
    "r2 = np.corrcoef(x, y)[0, 1] ** 2\n",
    "rmse = np.sqrt(np.mean((y - x) ** 2))\n",
    "\n",
    "# Add statistics text box\n",
    "stats_text = f\"R² = {r2:.2f}\\nRMSE = {rmse:.2f}°\\ny = {z[0]:.2f}x + {z[1]:.2f}\"\n",
    "ax2.text(\n",
    "    STATS_X_POS,\n",
    "    STATS_Y_POS,\n",
    "    stats_text,\n",
    "    transform=ax2.transAxes,\n",
    "    fontsize=TICK_LABELSIZE,\n",
    "    verticalalignment=\"top\",\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.0, edgecolor=\"grey\"),\n",
    ")\n",
    "\n",
    "ax2.grid(False)\n",
    "ax2.spines[\"top\"].set_visible(False)\n",
    "ax2.spines[\"right\"].set_visible(False)\n",
    "\n",
    "# Set tick spacing to every 5 degrees\n",
    "ax2.xaxis.set_major_locator(MultipleLocator(TICK_SPACING))\n",
    "ax2.yaxis.set_major_locator(MultipleLocator(TICK_SPACING))\n",
    "ax2.tick_params(axis=\"both\", which=\"major\", labelsize=TICK_LABELSIZE)\n",
    "ax2.set_aspect(\"equal\", adjustable=\"box\")\n",
    "ax2.tick_params(axis=\"y\", labelleft=False, length=6)\n",
    "\n",
    "# Align y-axis limits between both plots\n",
    "ax2.set_ylim(y_lim_min + 2, y_lim_max - 2)\n",
    "ax2.set_xlim(y_lim_min + 2, y_lim_max - 2)\n",
    "\n",
    "ax1.set_yticks([45, 50, 55, 60])\n",
    "ax2.set_yticks([45, 50, 55, 60])\n",
    "\n",
    "ax2.set_xlabel(\"AngleCam Prediction (°)\", fontsize=XLABEL_FONTSIZE)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(\n",
    "    FIGURE_OUTPUT_DIR / \"calathea_combined_plot.png\", dpi=600, bbox_inches=\"tight\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccafc442",
   "metadata": {},
   "source": [
    "#### Distributions AngleCam and TLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad03cc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_angle_distribution(data_list, target_time_str='18-01-2025 14:38'):\n",
    "    \"\"\"\n",
    "    Find the filename closest to the target time and return its angle_distribution.\n",
    "    \n",
    "    Args:\n",
    "        data_list: List of dictionaries with 'filename' and 'angle_distribution' keys\n",
    "        target_time_str: Target time in format 'DD-MM-YYYY HH:MM'\n",
    "    \n",
    "    Returns:\n",
    "        List: angle_distribution from the closest time match\n",
    "    \"\"\"\n",
    "    target_time = datetime.strptime(target_time_str, '%d-%m-%Y %H:%M')\n",
    "    \n",
    "    def parse_filename_timestamp(filename):\n",
    "        \"\"\"Parse timestamp from filename like 'Scan001_Calathea_250118_040802.las'\"\"\"\n",
    "        match = re.search(r'(\\d{6})_(\\d{6})', filename)\n",
    "        if not match:\n",
    "            return None\n",
    "        \n",
    "        date_part, time_part = match.groups()\n",
    "        year = 2000 + int(date_part[:2])\n",
    "        month = int(date_part[2:4])\n",
    "        day = int(date_part[4:6])\n",
    "        hour = int(time_part[:2])\n",
    "        minute = int(time_part[2:4])\n",
    "        second = int(time_part[4:6])\n",
    "        \n",
    "        return datetime(year, month, day, hour, minute, second)\n",
    "    \n",
    "    closest_item = None\n",
    "    min_diff = float('inf')\n",
    "    \n",
    "    for item in data_list:\n",
    "        file_time = parse_filename_timestamp(item['filename'])\n",
    "        if file_time is None:\n",
    "            continue\n",
    "        \n",
    "        time_diff = abs((file_time - target_time).total_seconds())\n",
    "        \n",
    "        if time_diff < min_diff:\n",
    "            min_diff = time_diff\n",
    "            closest_item = item\n",
    "    \n",
    "    if closest_item is None:\n",
    "        return None\n",
    "    \n",
    "    return closest_item['angle_distribution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db9e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted distribution from the results\n",
    "lad_calathea = calathea_results['predictions'][42][\"angle_distribution\"]\n",
    "lad_tls = get_closest_angle_distribution(calathea_tls_data['results'], '18-01-2025 14:38')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b86d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distributions(pred_lad, tls_lad, save_path=None):\n",
    "    \"\"\"Plot smoothed predicted vs TLS distributions\"\"\"\n",
    "    from scipy.interpolate import interp1d\n",
    "\n",
    "    # Get data\n",
    "    pred_dist = np.array(pred_lad)\n",
    "    tls_dist = np.array(tls_lad)\n",
    "\n",
    "    # Normalize\n",
    "    pred_dist = pred_dist / np.sum(pred_dist) if np.sum(pred_dist) > 0 else pred_dist\n",
    "    tls_dist = tls_dist / np.sum(tls_dist) if np.sum(tls_dist) > 0 else tls_dist\n",
    "\n",
    "    # Smooth slighty with moving average for better visualization\n",
    "    window = 3\n",
    "    pred_smooth = np.convolve(pred_dist, np.ones(window) / window, mode=\"same\")\n",
    "    tls_smooth = np.convolve(tls_dist, np.ones(window) / window, mode=\"same\")\n",
    "\n",
    "    # Interpolate\n",
    "    angles = np.linspace(0, 90, 43)\n",
    "    angles_fine = np.linspace(0, 90, 500)\n",
    "    pred_interp = interp1d(\n",
    "        angles, pred_smooth, kind=\"linear\", bounds_error=False, fill_value=0\n",
    "    )(angles_fine)\n",
    "    tls_interp = interp1d(\n",
    "        angles, tls_smooth, kind=\"linear\", bounds_error=False, fill_value=0\n",
    "    )(angles_fine)\n",
    "\n",
    "    # Plot\n",
    "    width = 5\n",
    "    fig, ax = plt.subplots(figsize=(width, width * 0.75))\n",
    "    ax.fill_between(angles_fine, tls_interp, alpha=0.4, color=custom_palette[1], label=\"TLS\")\n",
    "    ax.plot(angles_fine, tls_interp, color=custom_palette[1], linewidth=2.5)\n",
    "    ax.fill_between(\n",
    "        angles_fine, pred_interp, alpha=0.4, color=custom_palette2[3], label=\"AngleCam\"\n",
    "    )\n",
    "    ax.plot(angles_fine, pred_interp, color=custom_palette2[3], linewidth=2.5)\n",
    "\n",
    "    ax.legend(\n",
    "        fontsize=16,\n",
    "        frameon=False,\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(-0.02, 1.06),\n",
    "        handlelength=1.5,\n",
    "        handletextpad=0.35,\n",
    "    )\n",
    "    \n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.set_xlim(0, 87)\n",
    "    ax.set_ylim(0, None)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=16)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "save_path = FIGURE_OUTPUT_DIR / \"calathea_anglecam_tls_distribution_comparison.png\"\n",
    "\n",
    "# Usage\n",
    "plot_distributions(lad_calathea, lad_tls[2:], save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3428ade2",
   "metadata": {},
   "source": [
    "### Testing results maranta leuconeura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69926a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = prepare_plot_data(maranta_results)\n",
    "\n",
    "# Set all plotting parameters at the beginning for clarity and reproducibility\n",
    "FIGURE_SIZE = (22, 6)\n",
    "ROLLING_WINDOW = 3\n",
    "MARKER_SIZE = 80\n",
    "LINE_WIDTH = 5\n",
    "ALPHA_SCATTER = 0.7\n",
    "ALPHA_GRID = 0.2\n",
    "ROTATION_ANGLE = 0\n",
    "\n",
    "# Font sizes\n",
    "XLABEL_FONTSIZE = 18\n",
    "YLABEL_FONTSIZE = 18\n",
    "LEGEND_FONTSIZE = 18\n",
    "TICK_LABELSIZE = 18\n",
    "\n",
    "# Colors for time series\n",
    "ANGLECAM_COLOR = custom_palette2[3]\n",
    "ANGLECAM_LINE_COLOR = custom_palette2[3]\n",
    "TLS_COLOR = custom_palette[1]\n",
    "TLS_LINE_COLOR = custom_palette[1]\n",
    "\n",
    "# Day/Night visualization parameters\n",
    "NIGHT_COLOR = '#f0f0f0'  # Light grey for night background\n",
    "NIGHT_ALPHA = 1.0\n",
    "DAY_START_HOUR = 8  # 08:00 - when RGB images typically show daylight effects\n",
    "DAY_END_HOUR = 18   # 18:00 - when night period begins\n",
    "\n",
    "# Time axis formatting\n",
    "HOUR_INTERVALS = [0, 6, 12, 18]\n",
    "TIME_FORMAT = \"%H:%M\"\n",
    "\n",
    "# Scatter plot parameters\n",
    "SCATTER_MARKER_SIZE = 70\n",
    "SCATTER_ALPHA = 0.7\n",
    "ALPHA_REF_LINE = 0.99\n",
    "ALPHA_FIT_LINE = 0.9\n",
    "EDGE_LINE_WIDTH = 0.5\n",
    "REF_LINE_WIDTH = 3\n",
    "FIT_LINE_WIDTH = 3.5\n",
    "\n",
    "# Scatter plot colors\n",
    "SCATTER_COLOR = \"#595959\"\n",
    "REF_LINE_COLOR = \"#a5a5a5\"\n",
    "FIT_LINE_COLOR = \"black\"\n",
    "\n",
    "# Statistics text box positioning\n",
    "STATS_X_POS = 0.03\n",
    "STATS_Y_POS = 0.99\n",
    "\n",
    "# Line styles\n",
    "REF_LINE_STYLE = \"--\"\n",
    "FIT_LINE_STYLE = \"--\"\n",
    "\n",
    "# Tick spacing\n",
    "TICK_SPACING = 5\n",
    "\n",
    "# Regression line points for smooth consecutive line\n",
    "REGRESSION_POINTS = 100\n",
    "\n",
    "# Plot spacing and width ratios\n",
    "WIDTH_RATIOS = [2, 1]  # Time series : Scatter plot ratio\n",
    "SUBPLOT_SPACING = 0.001  # Distance between plots\n",
    "\n",
    "# Create figure with 1 row, 2 columns with custom width ratios and spacing\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    1,\n",
    "    2,\n",
    "    figsize=FIGURE_SIZE,\n",
    "    gridspec_kw={\"width_ratios\": WIDTH_RATIOS, \"wspace\": SUBPLOT_SPACING},\n",
    ")\n",
    "\n",
    "# LEFT PLOT: Time series with day/night background\n",
    "time_data = plot_df.dropna(subset=[\"timestamp\"]).sort_values(\"timestamp\")\n",
    "\n",
    "# Add day/night background shading\n",
    "time_min = time_data[\"timestamp\"].min()\n",
    "time_max = time_data[\"timestamp\"].max()\n",
    "\n",
    "# Create night period spans for each day in the dataset\n",
    "import pandas as pd\n",
    "from datetime import datetime, time\n",
    "\n",
    "# Get all unique dates in the dataset\n",
    "dates = pd.to_datetime(time_data[\"timestamp\"]).dt.date.unique()\n",
    "\n",
    "for date in dates:\n",
    "    # Convert date to datetime for the start of the day\n",
    "    day_start = pd.Timestamp.combine(date, time(0, 0))\n",
    "    day_end = pd.Timestamp.combine(date, time(23, 59, 59))\n",
    "    \n",
    "    # Define night periods: 00:00-08:00 and 18:00-24:00\n",
    "    night_start_early = day_start\n",
    "    night_end_early = pd.Timestamp.combine(date, time(DAY_START_HOUR, 0))\n",
    "    \n",
    "    night_start_late = pd.Timestamp.combine(date, time(DAY_END_HOUR, 0))\n",
    "    night_end_late = day_end\n",
    "    \n",
    "    # Only shade if the night period overlaps with our data range\n",
    "    if night_end_early >= time_min and night_start_early <= time_max:\n",
    "        ax1.axvspan(\n",
    "            max(night_start_early, time_min), \n",
    "            min(night_end_early, time_max),\n",
    "            color=NIGHT_COLOR, \n",
    "            alpha=NIGHT_ALPHA, \n",
    "            zorder=0\n",
    "        )\n",
    "    \n",
    "    if night_end_late >= time_min and night_start_late <= time_max:\n",
    "        ax1.axvspan(\n",
    "            max(night_start_late, time_min), \n",
    "            min(night_end_late, time_max),\n",
    "            color=NIGHT_COLOR, \n",
    "            alpha=NIGHT_ALPHA, \n",
    "            zorder=0\n",
    "        )\n",
    "\n",
    "# Plot TLS standard angles\n",
    "sns.scatterplot(\n",
    "    data=time_data,\n",
    "    x=\"timestamp\",\n",
    "    y=\"reference_mean_angle\",\n",
    "    marker=\"o\",\n",
    "    color=TLS_COLOR,\n",
    "    label=\"TLS\",\n",
    "    alpha=ALPHA_SCATTER,\n",
    "    s=MARKER_SIZE,\n",
    "    ax=ax1,\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "# Add smoothed TLS moving average\n",
    "time_data[\"reference_mean_angle_smooth\"] = (\n",
    "    time_data[\"reference_mean_angle\"]\n",
    "    .rolling(window=ROLLING_WINDOW, center=True, min_periods=1)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "# Plot the smoothed TLS moving average\n",
    "sns.lineplot(\n",
    "    data=time_data,\n",
    "    x=\"timestamp\",\n",
    "    y=\"reference_mean_angle_smooth\",\n",
    "    color=TLS_LINE_COLOR,\n",
    "    linewidth=LINE_WIDTH,\n",
    "    alpha=0.8,\n",
    "    ax=ax1,\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "# Plot predicted angles\n",
    "sns.scatterplot(\n",
    "    data=time_data,\n",
    "    x=\"timestamp\",\n",
    "    y=\"predicted_mean_angle\",\n",
    "    marker=\"o\",\n",
    "    color=ANGLECAM_COLOR,\n",
    "    label=\"AngleCam\",\n",
    "    alpha=ALPHA_SCATTER,\n",
    "    s=MARKER_SIZE,\n",
    "    ax=ax1,\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "# Calculate smoothed predicted mean angle\n",
    "time_data[\"predicted_mean_angle_smooth\"] = (\n",
    "    time_data[\"predicted_mean_angle\"]\n",
    "    .rolling(window=ROLLING_WINDOW, center=True, min_periods=1)\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "if len(time_data) > 0:\n",
    "    time_data.iloc[0, time_data.columns.get_loc(\"predicted_mean_angle_smooth\")] = (\n",
    "        time_data.iloc[0][\"predicted_mean_angle\"]\n",
    "    )\n",
    "    time_data.iloc[-1, time_data.columns.get_loc(\"predicted_mean_angle_smooth\")] = (\n",
    "        time_data.iloc[-1][\"predicted_mean_angle\"]\n",
    "    )\n",
    "\n",
    "sns.lineplot(\n",
    "    data=time_data,\n",
    "    x=\"timestamp\",\n",
    "    y=\"predicted_mean_angle_smooth\",\n",
    "    color=ANGLECAM_LINE_COLOR,\n",
    "    linewidth=LINE_WIDTH,\n",
    "    alpha=0.8,\n",
    "    ax=ax1,\n",
    "    zorder=3\n",
    ")\n",
    "\n",
    "ax1.grid(True, alpha=ALPHA_GRID)\n",
    "\n",
    "# Create custom legend that includes day/night explanation\n",
    "from matplotlib.patches import Rectangle\n",
    "legend_elements = ax1.get_legend_handles_labels()\n",
    "# Add night period indicator to legend\n",
    "night_patch = Rectangle((0, 0), 1, 1, facecolor=NIGHT_COLOR, alpha=NIGHT_ALPHA, \n",
    "                       edgecolor='grey', linewidth=0.5)\n",
    "legend_elements[0].append(night_patch)\n",
    "legend_elements[1].append('Night (18:00-08:00)')\n",
    "\n",
    "ax1.legend(\n",
    "    legend_elements[0], legend_elements[1],\n",
    "    fontsize=LEGEND_FONTSIZE, \n",
    "    frameon=False, \n",
    "    ncol=3, \n",
    "    borderaxespad=0, \n",
    "    handletextpad=0.0,\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0.5, 1.07)\n",
    ")\n",
    "\n",
    "# Set time formatting for x-axis\n",
    "ax1.xaxis.set_major_formatter(mdates.DateFormatter(TIME_FORMAT))\n",
    "ax1.xaxis.set_major_locator(mdates.HourLocator(byhour=HOUR_INTERVALS))\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=ROTATION_ANGLE)\n",
    "ax1.spines[\"top\"].set_visible(False)\n",
    "ax1.spines[\"right\"].set_visible(False)\n",
    "ax1.tick_params(axis=\"both\", which=\"major\", labelsize=TICK_LABELSIZE)\n",
    "\n",
    "ax1.set_xlabel(\"Time of Day\", fontsize=XLABEL_FONTSIZE)\n",
    "ax1.set_ylabel(\"Leaf Angle (°)\", fontsize=YLABEL_FONTSIZE)\n",
    "\n",
    "# Calculate y-axis limits for alignment\n",
    "y_min = min(\n",
    "    time_data[\"predicted_mean_angle_smooth\"].min(),\n",
    "    time_data[\"reference_mean_angle_smooth\"].min(),\n",
    ")\n",
    "y_max = max(\n",
    "    time_data[\"predicted_mean_angle_smooth\"].max(),\n",
    "    time_data[\"reference_mean_angle_smooth\"].max(),\n",
    ")\n",
    "y_margin = 0.25 * (y_max - y_min)\n",
    "y_lim_min = y_min - y_margin\n",
    "y_lim_max = y_max + y_margin\n",
    "\n",
    "ax1.set_ylim(y_lim_min + 2, y_lim_max - 2)\n",
    "\n",
    "# RIGHT PLOT: Scatter plot (unchanged)\n",
    "# Filter out any NaN values\n",
    "valid_data = plot_df.dropna(subset=[\"predicted_mean_angle\", \"reference_mean_angle\"])\n",
    "\n",
    "x = valid_data[\"predicted_mean_angle\"]  # AngleCam predictions on x-axis\n",
    "y = valid_data[\"reference_mean_angle\"]  # TLS on y-axis\n",
    "\n",
    "# Add 1:1 reference line\n",
    "min_val = min(x.min(), y.min())\n",
    "max_val = max(x.max(), y.max())\n",
    "ax2.plot(\n",
    "    [min_val - 1, max_val + 1],\n",
    "    [min_val - 1, max_val + 1],\n",
    "    REF_LINE_STYLE,\n",
    "    color=REF_LINE_COLOR,\n",
    "    alpha=ALPHA_REF_LINE,\n",
    "    linewidth=REF_LINE_WIDTH,\n",
    "    label=\"1:1 line\",\n",
    "    zorder=1,\n",
    ")\n",
    "\n",
    "# Create scatter plot\n",
    "ax2.scatter(\n",
    "    x,\n",
    "    y,\n",
    "    alpha=SCATTER_ALPHA,\n",
    "    s=SCATTER_MARKER_SIZE,\n",
    "    color=SCATTER_COLOR,\n",
    "    edgecolors=SCATTER_COLOR,\n",
    "    linewidth=EDGE_LINE_WIDTH,\n",
    ")\n",
    "\n",
    "# Add regression line - create consecutive x values for smooth line\n",
    "z = np.polyfit(x, y, 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(x.min(), x.max(), REGRESSION_POINTS)\n",
    "ax2.plot(\n",
    "    x_line,\n",
    "    p(x_line),\n",
    "    \"-\",\n",
    "    alpha=ALPHA_FIT_LINE,\n",
    "    linewidth=FIT_LINE_WIDTH,\n",
    "    label=f\"Fit: y = {z[0]:.2f}x + {z[1]:.2f}\",\n",
    "    color=FIT_LINE_COLOR,\n",
    ")\n",
    "\n",
    "# Calculate metrics\n",
    "r2 = np.corrcoef(x, y)[0, 1] ** 2\n",
    "rmse = np.sqrt(np.mean((y - x) ** 2))\n",
    "\n",
    "# Add statistics text box\n",
    "stats_text = f\"R² = {r2:.2f}\\nRMSE = {rmse:.2f}°\\ny = {z[0]:.2f}x + {z[1]:.2f}\"\n",
    "ax2.text(\n",
    "    STATS_X_POS,\n",
    "    STATS_Y_POS,\n",
    "    stats_text,\n",
    "    transform=ax2.transAxes,\n",
    "    fontsize=TICK_LABELSIZE,\n",
    "    verticalalignment=\"top\",\n",
    "    bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.0, edgecolor=\"grey\"),\n",
    ")\n",
    "\n",
    "ax2.grid(False)\n",
    "ax2.spines[\"top\"].set_visible(False)\n",
    "ax2.spines[\"right\"].set_visible(False)\n",
    "\n",
    "# Set tick spacing to every 5 degrees\n",
    "ax2.xaxis.set_major_locator(MultipleLocator(TICK_SPACING))\n",
    "ax2.yaxis.set_major_locator(MultipleLocator(TICK_SPACING))\n",
    "ax2.tick_params(axis=\"both\", which=\"major\", labelsize=TICK_LABELSIZE)\n",
    "ax2.set_aspect(\"equal\", adjustable=\"box\")\n",
    "ax2.tick_params(axis=\"y\", labelleft=False, length=6)\n",
    "\n",
    "# Align y-axis limits between both plots\n",
    "ax2.set_ylim(y_lim_min + 2, y_lim_max - 2)\n",
    "ax2.set_xlim(y_lim_min + 2, y_lim_max - 2)\n",
    "\n",
    "ax2.set_xlabel(\"AngleCam Prediction (°)\", fontsize=XLABEL_FONTSIZE)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(\n",
    "    FIGURE_OUTPUT_DIR / \"maranta_combined_plot.png\", dpi=600, bbox_inches=\"tight\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd276e74",
   "metadata": {},
   "source": [
    "#### Distributions AngleCam and TLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2759115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_angle_distribution(data_list, target_time_str='18-01-2025 14:38'):\n",
    "    \"\"\"\n",
    "    Find the filename closest to the target time and return its angle_distribution.\n",
    "    \n",
    "    Args:\n",
    "        data_list: List of dictionaries with 'filename' and 'angle_distribution' keys\n",
    "        target_time_str: Target time in format 'DD-MM-YYYY HH:MM'\n",
    "    \n",
    "    Returns:\n",
    "        List: angle_distribution from the closest time match\n",
    "    \"\"\"\n",
    "    target_time = datetime.strptime(target_time_str, '%d-%m-%Y %H:%M')\n",
    "    \n",
    "    def parse_filename_timestamp(filename):\n",
    "        \"\"\"Parse timestamp from filename like 'Scan001_Calathea_250118_040802.las'\"\"\"\n",
    "        match = re.search(r'(\\d{6})_(\\d{6})', filename)\n",
    "        if not match:\n",
    "            return None\n",
    "        \n",
    "        date_part, time_part = match.groups()\n",
    "        year = 2000 + int(date_part[:2])\n",
    "        month = int(date_part[2:4])\n",
    "        day = int(date_part[4:6])\n",
    "        hour = int(time_part[:2])\n",
    "        minute = int(time_part[2:4])\n",
    "        second = int(time_part[4:6])\n",
    "        \n",
    "        return datetime(year, month, day, hour, minute, second)\n",
    "    \n",
    "    closest_item = None\n",
    "    min_diff = float('inf')\n",
    "    \n",
    "    for item in data_list:\n",
    "        file_time = parse_filename_timestamp(item['filename'])\n",
    "        if file_time is None:\n",
    "            continue\n",
    "        \n",
    "        time_diff = abs((file_time - target_time).total_seconds())\n",
    "        \n",
    "        if time_diff < min_diff:\n",
    "            min_diff = time_diff\n",
    "            closest_item = item\n",
    "    \n",
    "    if closest_item is None:\n",
    "        return None\n",
    "    \n",
    "    return closest_item['angle_distribution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82db53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted distribution from the results\n",
    "lad_maranta = maranta_results['predictions'][109][\"angle_distribution\"]\n",
    "lad_tls = get_closest_angle_distribution(maranta_tls_data['results'], '20-01-2025 00:08')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d76c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distributions(pred_lad, tls_lad, save_path=None):\n",
    "    \"\"\"Plot smoothed predicted vs TLS distributions\"\"\"\n",
    "    from scipy.interpolate import interp1d\n",
    "\n",
    "    # Get data\n",
    "    pred_dist = np.array(pred_lad)\n",
    "    tls_dist = np.array(tls_lad)\n",
    "\n",
    "    # Normalize\n",
    "    pred_dist = pred_dist / np.sum(pred_dist) if np.sum(pred_dist) > 0 else pred_dist\n",
    "    tls_dist = tls_dist / np.sum(tls_dist) if np.sum(tls_dist) > 0 else tls_dist\n",
    "\n",
    "    # Smooth slighty with moving average for better visualization\n",
    "    window = 3\n",
    "    pred_smooth = np.convolve(pred_dist, np.ones(window) / window, mode=\"same\")\n",
    "    tls_smooth = np.convolve(tls_dist, np.ones(window) / window, mode=\"same\")\n",
    "\n",
    "    # Interpolate\n",
    "    angles = np.linspace(0, 90, 43)\n",
    "    angles_fine = np.linspace(0, 90, 500)\n",
    "    pred_interp = interp1d(\n",
    "        angles, pred_smooth, kind=\"linear\", bounds_error=False, fill_value=0\n",
    "    )(angles_fine)\n",
    "    tls_interp = interp1d(\n",
    "        angles, tls_smooth, kind=\"linear\", bounds_error=False, fill_value=0\n",
    "    )(angles_fine)\n",
    "\n",
    "    # Plot\n",
    "    width = 5\n",
    "    fig, ax = plt.subplots(figsize=(width, width * 0.75))\n",
    "    ax.fill_between(\n",
    "        angles_fine, tls_interp, alpha=0.4, color=custom_palette[1], label=\"TLS\"\n",
    "    )\n",
    "    ax.plot(angles_fine, tls_interp, color=custom_palette[1], linewidth=2.5)\n",
    "    ax.fill_between(\n",
    "        angles_fine, pred_interp, alpha=0.4, color=custom_palette2[3], label=\"AngleCam\"\n",
    "    )\n",
    "    ax.plot(angles_fine, pred_interp, color=custom_palette2[3], linewidth=2.5)\n",
    "\n",
    "    ax.legend(\n",
    "        fontsize=16,\n",
    "        frameon=False,\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(-0.02, 1.06),\n",
    "        handlelength=1.5,\n",
    "        handletextpad=0.35,\n",
    "    )\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.set_xlim(0, 87)\n",
    "    ax.set_ylim(0, None)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=16)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "save_path = FIGURE_OUTPUT_DIR / \"maranta_anglecam_tls_distribution_comparison.png\"\n",
    "\n",
    "# Usage\n",
    "plot_distributions(lad_maranta, lad_tls[2:], save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab60932c",
   "metadata": {},
   "source": [
    "### Testing results aglaonema commutatum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc2265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timestamp_from_filename(filename: str) -> datetime:\n",
    "    \"\"\"\n",
    "    Extract timestamp from filename pattern: G5Bullet_57_2024-12-24_00_49_45_corrected.jpg\n",
    "    Returns datetime object\n",
    "    \"\"\"\n",
    "    # Pattern to match: YYYY-MM-DD_HH_MM_SS\n",
    "    pattern = r'(\\d{4}-\\d{2}-\\d{2})_(\\d{2})_(\\d{2})_(\\d{2})'\n",
    "    match = re.search(pattern, filename)\n",
    "    \n",
    "    if match:\n",
    "        date_part = match.group(1)\n",
    "        hour = match.group(2)\n",
    "        minute = match.group(3)\n",
    "        second = match.group(4)\n",
    "        \n",
    "        timestamp_str = f\"{date_part} {hour}:{minute}:{second}\"\n",
    "        return datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    else:\n",
    "        raise ValueError(f\"Could not extract timestamp from filename: {filename}\")\n",
    "    \n",
    "extract_timestamp_from_filename(\"G5Bullet_57_2024-12-24_00_49_45_corrected.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c13ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timestamp_from_filename(filename: str) -> datetime:\n",
    "    \"\"\"\n",
    "    Extract timestamp from filename pattern: G5Bullet_57_2024-12-24_00_49_45_corrected.jpg\n",
    "    Returns datetime object\n",
    "    \"\"\"\n",
    "    # Pattern to match: YYYY-MM-DD_HH_MM_SS\n",
    "    pattern = r'(\\d{4}-\\d{2}-\\d{2})_(\\d{2})_(\\d{2})_(\\d{2})'\n",
    "    match = re.search(pattern, filename)\n",
    "    \n",
    "    if match:\n",
    "        date_part = match.group(1)\n",
    "        hour = match.group(2)\n",
    "        minute = match.group(3)\n",
    "        second = match.group(4)\n",
    "        \n",
    "        timestamp_str = f\"{date_part} {hour}:{minute}:{second}\"\n",
    "        return datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    else:\n",
    "        raise ValueError(f\"Could not extract timestamp from filename: {filename}\")\n",
    "    \n",
    "\n",
    "def add_linear_trend_analysis(time_data: pd.DataFrame, plot_trend: bool = True):\n",
    "    \"\"\"\n",
    "    Add linear trend analysis to time series data.\n",
    "    \n",
    "    Args:\n",
    "        time_data: DataFrame with timestamp and predicted_mean_angle columns\n",
    "        plot_trend: Whether to plot the trend line\n",
    "        \n",
    "    Returns:\n",
    "        dict: Trend analysis results\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "    \n",
    "    # Convert timestamps to numeric values for regression\n",
    "    time_numeric = pd.to_numeric(time_data['timestamp']) / 1e9  # Convert to seconds\n",
    "    angles = time_data['predicted_mean_angle'].values\n",
    "    \n",
    "    # Perform linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(time_numeric, angles)\n",
    "    \n",
    "    # Calculate trend line\n",
    "    trend_line = slope * time_numeric + intercept\n",
    "    time_data['linear_trend'] = trend_line\n",
    "    \n",
    "    # Convert slope to degrees per day\n",
    "    slope_per_day = slope * 86400  # seconds per day\n",
    "    \n",
    "    # Calculate trend statistics\n",
    "    trend_results = {\n",
    "        'slope_degrees_per_second': slope,\n",
    "        'slope_degrees_per_day': slope_per_day,\n",
    "        'intercept': intercept,\n",
    "        'r_squared': r_value**2,\n",
    "        'correlation': r_value,\n",
    "        'p_value': p_value,\n",
    "        'standard_error': std_err,\n",
    "        'trend_direction': 'increasing' if slope > 0 else 'decreasing' if slope < 0 else 'stable',\n",
    "        'significance': 'significant' if p_value < 0.05 else 'not significant'\n",
    "    }\n",
    "    \n",
    "    # Print trend summary\n",
    "    print(f\"\\n=== Linear Trend Analysis ===\")\n",
    "    print(f\"Slope: {slope_per_day:.3f} degrees/day\")\n",
    "    print(f\"R²: {trend_results['r_squared']:.3f}\")\n",
    "    print(f\"Correlation: {trend_results['correlation']:.3f}\")\n",
    "    print(f\"P-value: {p_value:.3e}\")\n",
    "    print(f\"Trend: {trend_results['trend_direction']} ({trend_results['significance']})\")\n",
    "    \n",
    "    return trend_results, time_data\n",
    "\n",
    "\n",
    "def create_time_series_plot(results_df: pd.DataFrame, output_path: str = \"\", title: str = \"AngleCam Time Series\"):\n",
    "    \"\"\"\n",
    "    Create time series plot from inference results with day/night visualization\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with inference results\n",
    "        output_path: Path to save the plot\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract timestamps and create time series data\n",
    "    time_data = results_df.copy()\n",
    "    time_data['timestamp'] = time_data['filename'].apply(extract_timestamp_from_filename)\n",
    "    time_data = time_data.sort_values('timestamp')\n",
    "    \n",
    "    # Add linear trend analysis\n",
    "    trend_results, time_data = add_linear_trend_analysis(time_data)\n",
    "    \n",
    "    # Set plotting parameters (same as reference)\n",
    "    FIGURE_SIZE = (14, 5)\n",
    "    ROLLING_WINDOW = 90*2\n",
    "    MARKER_SIZE = 12\n",
    "    LINE_WIDTH = 5\n",
    "    ALPHA_SCATTER = 0.2\n",
    "    ALPHA_GRID = 0.2\n",
    "    ROTATION_ANGLE = 15\n",
    "    \n",
    "    # Day/Night visualization parameters\n",
    "    NIGHT_COLOR = '#f0f0f0'  # Light grey for night background\n",
    "    NIGHT_ALPHA = 1.0\n",
    "    DAY_START_HOUR = 8  # 08:00 - when RGB images typically show daylight effects\n",
    "    DAY_END_HOUR = 18   # 18:00 - when night period begins\n",
    "    \n",
    "    # Font sizes\n",
    "    XLABEL_FONTSIZE = 18\n",
    "    YLABEL_FONTSIZE = 18\n",
    "    LEGEND_FONTSIZE = 14\n",
    "    TICK_LABELSIZE = 16\n",
    "    \n",
    "    # Colors\n",
    "    ANGLECAM_COLOR = custom_palette2[3]\n",
    "    ANGLECAM_LINE_COLOR = custom_palette2[3]\n",
    "    TREND_COLOR = 'black'  # Black for trend line\n",
    "    \n",
    "    # Time axis formatting\n",
    "    HOUR_INTERVALS = [0]\n",
    "    TIME_FORMAT = '%d-%m %H:%M'\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=FIGURE_SIZE)\n",
    "    \n",
    "    # Add day/night background shading\n",
    "    time_min = time_data[\"timestamp\"].min()\n",
    "    time_max = time_data[\"timestamp\"].max()\n",
    "    \n",
    "    # Create night period spans for each day in the dataset\n",
    "    import pandas as pd\n",
    "    from datetime import datetime, time\n",
    "    \n",
    "    # Get all unique dates in the dataset\n",
    "    dates = pd.to_datetime(time_data[\"timestamp\"]).dt.date.unique()\n",
    "    \n",
    "    for date in dates:\n",
    "        # Convert date to datetime for the start of the day\n",
    "        day_start = pd.Timestamp.combine(date, time(0, 0))\n",
    "        day_end = pd.Timestamp.combine(date, time(23, 59, 59))\n",
    "        \n",
    "        # Define night periods: 00:00-08:00 and 18:00-24:00\n",
    "        night_start_early = day_start\n",
    "        night_end_early = pd.Timestamp.combine(date, time(DAY_START_HOUR, 0))\n",
    "        \n",
    "        night_start_late = pd.Timestamp.combine(date, time(DAY_END_HOUR, 0))\n",
    "        night_end_late = day_end\n",
    "        \n",
    "        # Only shade if the night period overlaps with our data range\n",
    "        if night_end_early >= time_min and night_start_early <= time_max:\n",
    "            ax.axvspan(\n",
    "                max(night_start_early, time_min), \n",
    "                min(night_end_early, time_max),\n",
    "                color=NIGHT_COLOR, \n",
    "                alpha=NIGHT_ALPHA, \n",
    "                zorder=0\n",
    "            )\n",
    "        \n",
    "        if night_end_late >= time_min and night_start_late <= time_max:\n",
    "            ax.axvspan(\n",
    "                max(night_start_late, time_min), \n",
    "                min(night_end_late, time_max),\n",
    "                color=NIGHT_COLOR, \n",
    "                alpha=NIGHT_ALPHA, \n",
    "                zorder=0\n",
    "            )\n",
    "    \n",
    "    # Plot predicted angles (no label to avoid in legend)\n",
    "    sns.scatterplot(\n",
    "        data=time_data,\n",
    "        x='timestamp',\n",
    "        y='predicted_mean_angle',\n",
    "        marker='o',\n",
    "        color=ANGLECAM_COLOR,\n",
    "        edgecolor=ANGLECAM_COLOR,\n",
    "        alpha=ALPHA_SCATTER,\n",
    "        s=MARKER_SIZE,\n",
    "        ax=ax,\n",
    "        zorder=3\n",
    "    )\n",
    "    \n",
    "    # Calculate and plot smoothed predicted mean angle (no label to avoid in legend)\n",
    "    time_data['predicted_mean_angle_smooth'] = time_data['predicted_mean_angle'].rolling(\n",
    "        window=ROLLING_WINDOW, center=True, min_periods=1\n",
    "    ).mean()\n",
    "    \n",
    "    # Ensure first and last values are preserved\n",
    "    if len(time_data) > 0:\n",
    "        time_data.iloc[0, time_data.columns.get_loc('predicted_mean_angle_smooth')] = time_data.iloc[0]['predicted_mean_angle']\n",
    "        time_data.iloc[-1, time_data.columns.get_loc('predicted_mean_angle_smooth')] = time_data.iloc[-1]['predicted_mean_angle']\n",
    "    \n",
    "    sns.lineplot(\n",
    "        data=time_data,\n",
    "        x='timestamp',\n",
    "        y='predicted_mean_angle_smooth',\n",
    "        color=ANGLECAM_LINE_COLOR,\n",
    "        linewidth=LINE_WIDTH,\n",
    "        alpha=0.9,\n",
    "        ax=ax,\n",
    "        zorder=3\n",
    "    )\n",
    "    \n",
    "    # Plot linear trend line (no label to avoid in legend)\n",
    "    sns.lineplot(\n",
    "        data=time_data,\n",
    "        x='timestamp',\n",
    "        y='linear_trend',\n",
    "        color=TREND_COLOR,\n",
    "        linewidth=2.5,\n",
    "        alpha=0.85,\n",
    "        linestyle='-',\n",
    "        ax=ax,\n",
    "        zorder=3\n",
    "    )\n",
    "    \n",
    "    # Add regression statistics text box\n",
    "    n_samples = len(time_data)\n",
    "    r_squared = trend_results['r_squared']\n",
    "    slope_per_day = trend_results['slope_degrees_per_day']\n",
    "    intercept = trend_results['intercept']\n",
    "    p_value = trend_results['p_value']\n",
    "    \n",
    "    # Format equation\n",
    "    sign = '+' if intercept >= 0 else '-'\n",
    "    equation = f\"y = {slope_per_day:.3f}t {sign} {abs(intercept):.1f}\"\n",
    "    \n",
    "    # Create statistics text\n",
    "    stats_text = (\n",
    "        f\"slope = {slope_per_day:.2f} degrees/day\\n\"\n",
    "        f\"n = {n_samples}\\n\"\n",
    "    )\n",
    "    \n",
    "    # Add text box to plot\n",
    "    ax.text(0.01, 0.99, stats_text,\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=15,\n",
    "            verticalalignment='top',\n",
    "            horizontalalignment='left',\n",
    "            zorder=4)\n",
    "    \n",
    "    # Create custom legend with only night period indicator\n",
    "    from matplotlib.patches import Rectangle\n",
    "    night_patch = Rectangle((0, 0), 1, 1, facecolor=NIGHT_COLOR, alpha=NIGHT_ALPHA, \n",
    "                           edgecolor='grey', linewidth=0.5)\n",
    "    \n",
    "    # Create legend with only the night indicator\n",
    "    ax.legend([night_patch], ['Night (18:00-08:00)'], \n",
    "             loc='upper right', \n",
    "             fontsize=LEGEND_FONTSIZE, \n",
    "             frameon=False,\n",
    "             handletextpad=0.5,\n",
    "             bbox_to_anchor=(1.0, 1.025))\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Time', fontsize=XLABEL_FONTSIZE, labelpad=15)\n",
    "    ax.set_ylabel('Leaf Angle (°)', fontsize=YLABEL_FONTSIZE, labelpad=15)\n",
    "    ax.grid(True, alpha=ALPHA_GRID)\n",
    "    \n",
    "    # Set time formatting for x-axis\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(TIME_FORMAT))\n",
    "    ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=ROTATION_ANGLE)\n",
    "    \n",
    "    # Y axis ticks every 5 steps\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(5))\n",
    "    \n",
    "    # Style adjustments\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.tick_params(axis='x', which='major', labelsize=14)\n",
    "    ax.tick_params(axis='y', which='major', labelsize=TICK_LABELSIZE)\n",
    "    \n",
    "    # Set y-axis limits with margin\n",
    "    y_min = time_data['predicted_mean_angle'].min()\n",
    "    y_max = time_data['predicted_mean_angle'].max()\n",
    "    y_margin = 0.05 * (y_max - y_min)\n",
    "    ax.set_ylim(y_min - y_margin, y_max + y_margin)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    \n",
    "    # Save if path provided\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, dpi=400, bbox_inches='tight')\n",
    "        print(f\"Plot saved to: {output_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return time_data\n",
    "\n",
    "# Create the time series plot for Aglaonema data\n",
    "output_plot_path = FIGURE_OUTPUT_DIR / \"aglaonema_anglecam_time_series.png\"\n",
    "\n",
    "# Make sure the output directory exists\n",
    "os.makedirs(os.path.dirname(output_plot_path), exist_ok=True)\n",
    "\n",
    "results_df = pd.DataFrame(aglaonema_results[\"predictions\"])\n",
    "\n",
    "time_data = create_time_series_plot(\n",
    "    results_df=results_df,\n",
    "    output_path=output_plot_path,\n",
    "    title=\"Aglaonema AngleCam Time Series\"\n",
    ")\n",
    "\n",
    "print(f\"Time series data shape: {time_data.shape}\")\n",
    "print(f\"Time range: {time_data['timestamp'].min()} to {time_data['timestamp'].max()}\")\n",
    "print(f\"Angle range: {time_data['predicted_mean_angle'].min():.1f}° to {time_data['predicted_mean_angle'].max():.1f}°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6dfba9",
   "metadata": {},
   "source": [
    "#### Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d6fff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean distribution of all predictions\n",
    "def calculate_mean_distribution(results_df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Calculate the mean distribution across all predicted distributions.\"\"\"\n",
    "    valid_distributions = []\n",
    "    for dist in results_df[\"angle_distribution\"]:\n",
    "        if dist is not None and len(dist) == 43:\n",
    "            valid_distributions.append(dist)\n",
    "    \n",
    "    if len(valid_distributions) == 0:\n",
    "        print(\"Warning: No valid distributions found!\")\n",
    "        return np.zeros(43)\n",
    "    \n",
    "    all_distributions = np.array(valid_distributions)\n",
    "    mean_distribution = np.mean(all_distributions, axis=0)\n",
    "    mean_distribution = mean_distribution / np.sum(mean_distribution)  # Normalize\n",
    "    \n",
    "    print(f\"Calculated mean from {len(valid_distributions)} valid distributions\")\n",
    "    return mean_distribution\n",
    "\n",
    "# Calculate once\n",
    "mean_distribution = calculate_mean_distribution(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d16dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_entry(\n",
    "    results_df: pd.DataFrame, target_datetime: datetime\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Find the entry closest to the target datetime.\n",
    "\n",
    "    Args:\n",
    "        results_df: DataFrame with inference results and timestamps\n",
    "        target_datetime: Target datetime to find closest entry for\n",
    "\n",
    "    Returns:\n",
    "        Series containing the closest entry data\n",
    "    \"\"\"\n",
    "    # Extract timestamps if not already done\n",
    "    if \"timestamp\" not in results_df.columns:\n",
    "        results_df[\"timestamp\"] = results_df[\"filename\"].apply(\n",
    "            extract_timestamp_from_filename\n",
    "        )\n",
    "\n",
    "    # Calculate time differences\n",
    "    time_diffs = abs(results_df[\"timestamp\"] - target_datetime)\n",
    "    closest_idx = time_diffs.idxmin()\n",
    "\n",
    "    closest_entry = results_df.loc[closest_idx]\n",
    "    actual_time = closest_entry[\"timestamp\"]\n",
    "    time_diff = abs(actual_time - target_datetime).total_seconds() / 60  # in minutes\n",
    "\n",
    "    print(f\"Target time: {target_datetime}\")\n",
    "    print(f\"Closest entry: {actual_time} (diff: {time_diff:.1f} minutes)\")\n",
    "    print(f\"Filename: {closest_entry['filename']}\")\n",
    "\n",
    "    return closest_entry\n",
    "\n",
    "\n",
    "def plot_distribution(\n",
    "    results_df: pd.DataFrame,\n",
    "    target_datetimes: List[datetime],\n",
    "    mean_distribution: np.ndarray = None,\n",
    "    output_path: str = \"\",\n",
    "    color: str = None,\n",
    "    plot_y_axis: bool = True,\n",
    "    y_ticks: bool = True,\n",
    "    window_size: int = 5,\n",
    ") -> None:\n",
    "    from scipy.interpolate import interp1d\n",
    "\n",
    "    # Set plotting parameters\n",
    "    width = 4\n",
    "    FIGURE_SIZE = (width, width * 0.75)\n",
    "    TICK_LABELSIZE = 16\n",
    "\n",
    "    angles = np.linspace(0, 90, 43)\n",
    "\n",
    "    # Calculate mean angle from provided mean_distribution (if available)\n",
    "    if mean_distribution is not None:\n",
    "        mean_angle_overall = np.sum(mean_distribution * angles)\n",
    "\n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=FIGURE_SIZE)\n",
    "\n",
    "    for i, target_time in enumerate(target_datetimes):\n",
    "        closest_entry = find_closest_entry(results_df, target_time)\n",
    "        distribution = np.array(closest_entry[\"angle_distribution\"])\n",
    "        predicted_mean = closest_entry[\"predicted_mean_angle\"]\n",
    "\n",
    "        # Apply moving average smoothing\n",
    "        distribution_ma = np.convolve(\n",
    "            distribution, np.ones(window_size) / window_size, mode=\"same\"\n",
    "        )\n",
    "\n",
    "        f_interp = interp1d(\n",
    "            angles, distribution_ma, kind=\"quadratic\", bounds_error=False, fill_value=0\n",
    "        )\n",
    "\n",
    "        # Create smooth points\n",
    "        angles_smooth = np.linspace(0, 90, 500)\n",
    "        distribution_smooth = f_interp(angles_smooth)\n",
    "        distribution_smooth = np.maximum(distribution_smooth, 0)\n",
    "\n",
    "        time_label = closest_entry[\"timestamp\"].strftime(\"%H:%M\")\n",
    "\n",
    "        # Plot filled area\n",
    "        ax.fill_between(\n",
    "            angles_smooth,\n",
    "            distribution_smooth,\n",
    "            alpha=0.7,\n",
    "            color=color,\n",
    "            label=f\"{time_label} (mean: {predicted_mean:.1f}°)\",\n",
    "        )\n",
    "\n",
    "        # Add outline\n",
    "        ax.plot(\n",
    "            angles_smooth, distribution_smooth, color=color, linewidth=2.5, alpha=1.0\n",
    "        )\n",
    "\n",
    "        # Add mean line\n",
    "        ax.axvline(\n",
    "            x=predicted_mean, color=color, linestyle=\"--\", linewidth=2, alpha=0.8\n",
    "        )\n",
    "\n",
    "    # Add mean distribution as dashed line\n",
    "    if mean_distribution is not None:\n",
    "        # Apply same smoothing to mean distribution\n",
    "        mean_distribution_ma = np.convolve(\n",
    "            mean_distribution, np.ones(5) / 5, mode=\"same\"\n",
    "        )\n",
    "        f_interp_mean = interp1d(\n",
    "            angles,\n",
    "            mean_distribution_ma,\n",
    "            kind=\"quadratic\",\n",
    "            bounds_error=False,\n",
    "            fill_value=0,\n",
    "        )\n",
    "        mean_distribution_smooth = f_interp_mean(angles_smooth)\n",
    "        mean_distribution_smooth = np.maximum(mean_distribution_smooth, 0)\n",
    "\n",
    "        ax.plot(\n",
    "            angles_smooth,\n",
    "            mean_distribution_smooth,\n",
    "            color=\"black\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=3,\n",
    "            alpha=0.9,\n",
    "            label=f\"Mean distribution ({mean_angle_overall:.1f}°)\",\n",
    "        )\n",
    "\n",
    "    # Formatting\n",
    "    ax.plot([0, 90], [0, 0], color=\"black\", linewidth=2)\n",
    "    #ax.grid(True, alpha=0.2)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.tick_params(axis=\"x\", which=\"major\", labelsize=14)\n",
    "    ax.tick_params(axis=\"y\", which=\"major\", labelsize=TICK_LABELSIZE)\n",
    "    ax.set_xlim(0, 90)\n",
    "    ax.set_ylim(0, None)\n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(10))\n",
    "    \n",
    "    # Set only 5 y-axis ticks\n",
    "    ax.yaxis.set_major_locator(plt.MultipleLocator(0.01))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if output_path:\n",
    "        plt.savefig(output_path, dpi=400, bbox_inches=\"tight\")\n",
    "        print(f\"Plot saved to: {output_path}\")\n",
    "        \n",
    "    print(f\"Mean angle: {predicted_mean:.1f}°\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13936b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plot\n",
    "target_time = datetime(2024, 12, 17, 1, 30, 0)\n",
    "output_path_single = FIGURE_OUTPUT_DIR / f\"aglaonema_distribution_{target_time.strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "\n",
    "plot_distribution(\n",
    "    results_df=results_df,\n",
    "    target_datetimes=[target_time],\n",
    "    mean_distribution=mean_distribution,\n",
    "    output_path=output_path_single,\n",
    "    color=custom_palette[3],\n",
    "    window_size=2,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8020fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plot\n",
    "target_time = datetime(2024, 12, 22, 11, 30, 0)\n",
    "output_path_single = FIGURE_OUTPUT_DIR / f\"aglaonema_distribution_{target_time.strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "\n",
    "plot_distribution(\n",
    "    results_df=results_df,\n",
    "    target_datetimes=[target_time],\n",
    "    mean_distribution=mean_distribution,\n",
    "    output_path=output_path_single,\n",
    "    color=custom_palette2[3],\n",
    "    window_size=2,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb65d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plot\n",
    "target_time = datetime(2024, 12, 26, 0, 0, 0)\n",
    "output_path_single = FIGURE_OUTPUT_DIR / f\"aglaonema_distribution_{target_time.strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "\n",
    "plot_distribution(\n",
    "    results_df=results_df,\n",
    "    target_datetimes=[target_time],\n",
    "    mean_distribution=mean_distribution,\n",
    "    output_path=output_path_single,\n",
    "    color=custom_palette[3],\n",
    "    window_size=2,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c714a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution plot\n",
    "target_time = datetime(2024, 12, 31, 14, 30, 0)\n",
    "output_path_single = FIGURE_OUTPUT_DIR / f\"aglaonema_distribution_{target_time.strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "\n",
    "plot_distribution(\n",
    "    results_df=results_df,\n",
    "    target_datetimes=[target_time],\n",
    "    mean_distribution=mean_distribution,\n",
    "    output_path=output_path_single,\n",
    "    color=custom_palette2[3],\n",
    "    window_size=2,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67125fb5",
   "metadata": {},
   "source": [
    "#### LAD variability trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6998fd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "aglaonema_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33419c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distribution_variability_from_dict(results_dict: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate various measures of LAD variability over time from loaded results dictionary.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dictionary containing predictions (already loaded from JSON)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with timestamp and variability measures\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    import re\n",
    "    from scipy.stats import entropy, skew, kurtosis\n",
    "    \n",
    "    def extract_timestamp_from_filename(filename: str) -> datetime:\n",
    "        \"\"\"\n",
    "        Extract timestamp from filename pattern: G5Bullet_57_2024-12-24_00_49_45_corrected.jpg\n",
    "        Returns datetime object\n",
    "        \"\"\"\n",
    "        # Pattern to match: YYYY-MM-DD_HH_MM_SS\n",
    "        pattern = r'(\\d{4}-\\d{2}-\\d{2})_(\\d{2})_(\\d{2})_(\\d{2})'\n",
    "        match = re.search(pattern, filename)\n",
    "        \n",
    "        if match:\n",
    "            date_part = match.group(1)\n",
    "            hour = match.group(2)\n",
    "            minute = match.group(3)\n",
    "            second = match.group(4)\n",
    "            \n",
    "            timestamp_str = f\"{date_part} {hour}:{minute}:{second}\"\n",
    "            return datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            raise ValueError(f\"Could not extract timestamp from filename: {filename}\")\n",
    "    \n",
    "    # Extract predictions from the dictionary\n",
    "    predictions = results_dict['predictions']\n",
    "    print(f\"Found {len(predictions)} predictions in results dictionary\")\n",
    "    \n",
    "    variability_data = []\n",
    "    angles = np.linspace(0, 90, 43)  # AngleCam uses 43 bins from 0° to 90°\n",
    "    \n",
    "    print(\"Processing predictions...\")\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        if i % 1000 == 0:  # Progress indicator\n",
    "            print(f\"Processed {i}/{len(predictions)} predictions...\")\n",
    "        \n",
    "        try:\n",
    "            # Extract data from prediction\n",
    "            filename = prediction['filename']\n",
    "            mean_angle = prediction['predicted_mean_angle']\n",
    "            distribution = np.array(prediction['angle_distribution'])\n",
    "            \n",
    "            # Ensure distribution is normalized (should sum to 1)\n",
    "            if np.sum(distribution) > 0:\n",
    "                distribution = distribution / np.sum(distribution)\n",
    "            else:\n",
    "                print(f\"Warning: Empty distribution for {filename}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate various variability measures\n",
    "            variance = np.sum(distribution * (angles - mean_angle) ** 2)\n",
    "            std_dev = np.sqrt(variance)\n",
    "            \n",
    "            # Distribution entropy (higher = more uniform distribution)\n",
    "            dist_entropy = entropy(distribution + 1e-10)  # Add small epsilon to avoid log(0)\n",
    "            \n",
    "            # Distribution skewness (shape asymmetry)\n",
    "            dist_skewness = skew(distribution)\n",
    "            \n",
    "            # Distribution kurtosis (tail heaviness)\n",
    "            dist_kurtosis = kurtosis(distribution)\n",
    "            \n",
    "            # 95% confidence interval width\n",
    "            try:\n",
    "                cumulative = np.cumsum(distribution)\n",
    "                lower_idx = np.where(cumulative >= 0.025)[0][0]\n",
    "                upper_idx = np.where(cumulative >= 0.975)[0][0]\n",
    "                ci_width = angles[upper_idx] - angles[lower_idx]\n",
    "            except (IndexError, ValueError):\n",
    "                ci_width = np.nan\n",
    "            \n",
    "            # Interquartile range\n",
    "            try:\n",
    "                cumulative = np.cumsum(distribution)\n",
    "                q25_idx = np.where(cumulative >= 0.25)[0][0]\n",
    "                q75_idx = np.where(cumulative >= 0.75)[0][0]\n",
    "                iqr = angles[q75_idx] - angles[q25_idx]\n",
    "            except (IndexError, ValueError):\n",
    "                iqr = np.nan\n",
    "            \n",
    "            # Peak height (maximum probability density)\n",
    "            peak_height = np.max(distribution)\n",
    "            \n",
    "            # Mode (angle with highest probability)\n",
    "            mode_idx = np.argmax(distribution)\n",
    "            mode_angle = angles[mode_idx]\n",
    "            \n",
    "            # Extract timestamp from filename\n",
    "            timestamp = extract_timestamp_from_filename(filename)\n",
    "            \n",
    "            variability_data.append({\n",
    "                'timestamp': timestamp,\n",
    "                'filename': filename,\n",
    "                'mean_angle': mean_angle,\n",
    "                'std_dev': std_dev,\n",
    "                'variance': variance,\n",
    "                'entropy': dist_entropy,\n",
    "                'skewness': dist_skewness,\n",
    "                'kurtosis': dist_kurtosis,\n",
    "                'ci_width': ci_width,\n",
    "                'iqr': iqr,\n",
    "                'peak_height': peak_height,\n",
    "                'mode_angle': mode_angle\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing prediction {i} ({filename}): {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Successfully processed {len(variability_data)} predictions\")\n",
    "    \n",
    "    # Convert to DataFrame and sort by timestamp\n",
    "    df = pd.DataFrame(variability_data)\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Time range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "    print(f\"Mean angle range: {df['mean_angle'].min():.1f}° to {df['mean_angle'].max():.1f}°\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d28629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lad_variability_time_series(variability_df: pd.DataFrame, output_dir: Path):\n",
    "    \"\"\"\n",
    "    Create comprehensive time series plots showing how LAD shape changes over time.\n",
    "    \"\"\"\n",
    "    # Set up the plot parameters\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(16, 12))\n",
    "    fig.suptitle('LAD Variability Changes During Water Stress', fontsize=16, y=0.95)\n",
    "    \n",
    "    # Colors\n",
    "    primary_color = custom_palette2[3]\n",
    "    trend_color = 'black'\n",
    "    \n",
    "    # Common plotting parameters\n",
    "    marker_size = 8\n",
    "    alpha_points = 0.3\n",
    "    line_width = 2\n",
    "    trend_line_width = 2\n",
    "    \n",
    "    # Calculate rolling averages for smoother lines\n",
    "    window = 120  # 4-hour window (every 2 minutes = 120 points)\n",
    "    \n",
    "    # Helper function to add trend line and statistics\n",
    "    def add_trend_analysis(ax, x, y, label):\n",
    "        from scipy import stats\n",
    "        # Convert timestamps to numeric for regression\n",
    "        x_numeric = pd.to_numeric(x) / 1e9\n",
    "        valid_mask = ~np.isnan(y)\n",
    "        if valid_mask.sum() > 10:  # Need at least 10 points\n",
    "            slope, intercept, r_value, p_value, _ = stats.linregress(\n",
    "                x_numeric[valid_mask], y[valid_mask]\n",
    "            )\n",
    "            \n",
    "            # Plot trend line\n",
    "            trend_line = slope * x_numeric + intercept\n",
    "            ax.plot(x, trend_line, color=trend_color, linewidth=trend_line_width, \n",
    "                   alpha=0.8, linestyle='--')\n",
    "            \n",
    "            # Add R² to the plot\n",
    "            ax.text(0.02, 0.98, f'R² = {r_value**2:.3f} (p = {p_value:.3f}), \\nr = {r_value:.3f}', \n",
    "                   transform=ax.transAxes, fontsize=10,\n",
    "                   verticalalignment='top', \n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "    \n",
    "    # 1. Standard Deviation (measure of spread)\n",
    "    ax = axes[0, 0]\n",
    "    y_smooth = variability_df['std_dev'].rolling(window=window, center=True).mean()\n",
    "    ax.scatter(variability_df['timestamp'], variability_df['std_dev'], \n",
    "              alpha=alpha_points, s=marker_size, color=primary_color)\n",
    "    ax.plot(variability_df['timestamp'], y_smooth, color=primary_color, linewidth=line_width)\n",
    "    add_trend_analysis(ax, variability_df['timestamp'], variability_df['std_dev'], 'std_dev')\n",
    "    ax.set_title('Standard Deviation')\n",
    "    ax.set_ylabel('Std Dev (degrees)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Entropy (uniformity of distribution)\n",
    "    ax = axes[0, 1]\n",
    "    y_smooth = variability_df['entropy'].rolling(window=window, center=True).mean()\n",
    "    ax.scatter(variability_df['timestamp'], variability_df['entropy'], \n",
    "              alpha=alpha_points, s=marker_size, color=primary_color)\n",
    "    ax.plot(variability_df['timestamp'], y_smooth, color=primary_color, linewidth=line_width)\n",
    "    add_trend_analysis(ax, variability_df['timestamp'], variability_df['entropy'], 'entropy')\n",
    "    ax.set_title('Distribution Entropy')\n",
    "    ax.set_ylabel('Entropy')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. 95% Confidence Interval Width\n",
    "    ax = axes[1, 0]\n",
    "    y_smooth = variability_df['ci_width'].rolling(window=window, center=True).mean()\n",
    "    ax.scatter(variability_df['timestamp'], variability_df['ci_width'], \n",
    "              alpha=alpha_points, s=marker_size, color=primary_color)\n",
    "    ax.plot(variability_df['timestamp'], y_smooth, color=primary_color, linewidth=line_width)\n",
    "    add_trend_analysis(ax, variability_df['timestamp'], variability_df['ci_width'], 'ci_width')\n",
    "    ax.set_title('95% Confidence Interval Width')\n",
    "    ax.set_ylabel('CI Width (degrees)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Skewness (distribution asymmetry)\n",
    "    ax = axes[1, 1]\n",
    "    y_smooth = variability_df['skewness'].rolling(window=window, center=True).mean()\n",
    "    ax.scatter(variability_df['timestamp'], variability_df['skewness'], \n",
    "              alpha=alpha_points, s=marker_size, color=primary_color)\n",
    "    ax.plot(variability_df['timestamp'], y_smooth, color=primary_color, linewidth=line_width)\n",
    "    add_trend_analysis(ax, variability_df['timestamp'], variability_df['skewness'], 'skewness')\n",
    "    ax.set_title('Distribution Skewness')\n",
    "    ax.set_ylabel('Skewness')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='gray', linestyle=':', alpha=0.7)  # Reference line\n",
    "    \n",
    "    # 5. Peak Height (concentration)\n",
    "    ax = axes[2, 0]\n",
    "    y_smooth = variability_df['peak_height'].rolling(window=window, center=True).mean()\n",
    "    ax.scatter(variability_df['timestamp'], variability_df['peak_height'], \n",
    "              alpha=alpha_points, s=marker_size, color=primary_color)\n",
    "    ax.plot(variability_df['timestamp'], y_smooth, color=primary_color, linewidth=line_width)\n",
    "    add_trend_analysis(ax, variability_df['timestamp'], variability_df['peak_height'], 'peak_height')\n",
    "    ax.set_title('Peak Height (Max Probability)')\n",
    "    ax.set_ylabel('Peak Height')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Mean Angle (for comparison)\n",
    "    ax = axes[2, 1]\n",
    "    y_smooth = variability_df['mean_angle'].rolling(window=window, center=True).mean()\n",
    "    ax.scatter(variability_df['timestamp'], variability_df['mean_angle'], \n",
    "              alpha=alpha_points, s=marker_size, color=primary_color)\n",
    "    ax.plot(variability_df['timestamp'], y_smooth, color=primary_color, linewidth=line_width)\n",
    "    add_trend_analysis(ax, variability_df['timestamp'], variability_df['mean_angle'], 'mean_angle')\n",
    "    ax.set_title('Mean Angle (Reference)')\n",
    "    ax.set_ylabel('Mean Angle (degrees)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. Mode Angle (for comparison)\n",
    "    ax = axes[3, 0]\n",
    "    y_smooth = variability_df['mode_angle'].rolling(window=window, center=True).mean()\n",
    "    ax.scatter(variability_df['timestamp'], variability_df['mode_angle'], \n",
    "              alpha=alpha_points, s=marker_size, color=primary_color)\n",
    "    ax.plot(variability_df['timestamp'], y_smooth, color=primary_color, linewidth=line_width)\n",
    "    add_trend_analysis(ax, variability_df['timestamp'], variability_df['mode_angle'], 'mode_angle')\n",
    "    \n",
    "    # 8. Variance (for comparison)\n",
    "    ax = axes[3, 1]\n",
    "    y_smooth = variability_df['variance'].rolling(window=window, center=True).mean()\n",
    "    ax.scatter(variability_df['timestamp'], variability_df['variance'], \n",
    "              alpha=alpha_points, s=marker_size, color=primary_color)\n",
    "    ax.plot(variability_df['timestamp'], y_smooth, color=primary_color, linewidth=line_width)\n",
    "    add_trend_analysis(ax, variability_df['timestamp'], variability_df['variance'], 'variance')\n",
    "    \n",
    "    # Format x-axis for all subplots\n",
    "    for ax in axes.flat:\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))\n",
    "        ax.xaxis.set_major_locator(mdates.DayLocator(interval=2))\n",
    "        plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "        ax.tick_params(labelsize=9)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    output_path = output_dir / \"aglaonema_lad_variability_analysis.png\"\n",
    "    plt.savefig(output_path, dpi=400, bbox_inches='tight')\n",
    "    print(f\"Variability analysis plot saved to: {output_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_distribution_evolution_heatmap(results_dict: dict, output_dir: Path):\n",
    "    \"\"\"\n",
    "    Create a heatmap showing how the entire LAD evolves over time.\n",
    "    \n",
    "    Args:\n",
    "        results_dict: Dictionary containing predictions (already loaded from JSON)\n",
    "        output_dir: Path object for output directory\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import re\n",
    "    \n",
    "    def extract_timestamp_from_filename(filename: str) -> datetime:\n",
    "        \"\"\"\n",
    "        Extract timestamp from filename pattern: G5Bullet_57_2024-12-24_00_49_45_corrected.jpg\n",
    "        Returns datetime object\n",
    "        \"\"\"\n",
    "        # Pattern to match: YYYY-MM-DD_HH_MM_SS\n",
    "        pattern = r'(\\d{4}-\\d{2}-\\d{2})_(\\d{2})_(\\d{2})_(\\d{2})'\n",
    "        match = re.search(pattern, filename)\n",
    "        \n",
    "        if match:\n",
    "            date_part = match.group(1)\n",
    "            hour = match.group(2)\n",
    "            minute = match.group(3)\n",
    "            second = match.group(4)\n",
    "            \n",
    "            timestamp_str = f\"{date_part} {hour}:{minute}:{second}\"\n",
    "            return datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            raise ValueError(f\"Could not extract timestamp from filename: {filename}\")\n",
    "    \n",
    "    # Extract predictions from the dictionary\n",
    "    predictions = results_dict['predictions']\n",
    "    print(f\"Processing {len(predictions)} predictions for heatmap...\")\n",
    "    \n",
    "    # Sample every 60 points (2 hours) to reduce density\n",
    "    sample_step = 60\n",
    "    sampled_predictions = predictions[::sample_step]\n",
    "    print(f\"Sampled to {len(sampled_predictions)} points for visualization\")\n",
    "    \n",
    "    # Extract timestamps and distributions\n",
    "    timestamps = []\n",
    "    distributions = []\n",
    "    \n",
    "    for i, prediction in enumerate(sampled_predictions):\n",
    "        try:\n",
    "            filename = prediction['filename']\n",
    "            timestamp = extract_timestamp_from_filename(filename)\n",
    "            distribution = np.array(prediction['angle_distribution'])\n",
    "            \n",
    "            # Normalize distribution\n",
    "            if np.sum(distribution) > 0:\n",
    "                distribution = distribution / np.sum(distribution)\n",
    "            else:\n",
    "                print(f\"Warning: Empty distribution for {filename}\")\n",
    "                continue\n",
    "            \n",
    "            timestamps.append(timestamp)\n",
    "            distributions.append(distribution)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing prediction {i}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if len(distributions) == 0:\n",
    "        print(\"Error: No valid distributions found!\")\n",
    "        return\n",
    "    \n",
    "    # Create the distribution matrix\n",
    "    dist_matrix = np.array(distributions).T  # Shape: (43 angles, n_timepoints)\n",
    "    print(f\"Created distribution matrix: {dist_matrix.shape}\")\n",
    "    \n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Create heatmap\n",
    "    angles = np.linspace(0, 90, 43)\n",
    "    im = ax.imshow(dist_matrix, aspect='auto', cmap='viridis', origin='lower')\n",
    "    \n",
    "    # Set y-axis (angles)\n",
    "    angle_ticks = np.arange(0, 43, 5)\n",
    "    angle_labels = [f\"{angles[i]:.0f}°\" for i in angle_ticks]\n",
    "    ax.set_yticks(angle_ticks)\n",
    "    ax.set_yticklabels(angle_labels)\n",
    "    ax.set_ylabel('Leaf Angle (degrees)', fontsize=12)\n",
    "    \n",
    "    # Set x-axis (time)\n",
    "    n_time_ticks = 8\n",
    "    if len(timestamps) > 1:\n",
    "        time_tick_indices = np.linspace(0, len(timestamps)-1, n_time_ticks, dtype=int)\n",
    "        time_labels = [timestamps[i].strftime('%m-%d') for i in time_tick_indices]\n",
    "        ax.set_xticks(time_tick_indices)\n",
    "        ax.set_xticklabels(time_labels, rotation=45)\n",
    "    else:\n",
    "        ax.set_xticks([0])\n",
    "        ax.set_xticklabels([timestamps[0].strftime('%m-%d')])\n",
    "    \n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "    cbar.set_label('Probability Density', fontsize=12)\n",
    "    \n",
    "    # Title and formatting\n",
    "    ax.set_title('LAD Evolution During Water Stress (Aglaonema)', fontsize=14, pad=20)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the plot\n",
    "    output_path = output_dir / \"aglaonema_lad_evolution_heatmap.png\"\n",
    "    plt.savefig(output_path, dpi=400, bbox_inches='tight')\n",
    "    print(f\"LAD evolution heatmap saved to: {output_path}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86acb453",
   "metadata": {},
   "outputs": [],
   "source": [
    "variability_df = calculate_distribution_variability_from_dict(aglaonema_results)\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n=== Variability Statistics Summary ===\")\n",
    "print(f\"Standard deviation: {variability_df['std_dev'].mean():.2f} ± {variability_df['std_dev'].std():.2f}\")\n",
    "print(f\"Entropy: {variability_df['entropy'].mean():.3f} ± {variability_df['entropy'].std():.3f}\")\n",
    "print(f\"95% CI width: {variability_df['ci_width'].mean():.1f}° ± {variability_df['ci_width'].std():.1f}°\")\n",
    "print(f\"Skewness: {variability_df['skewness'].mean():.3f} ± {variability_df['skewness'].std():.3f}\")\n",
    "print(f\"Peak height: {variability_df['peak_height'].mean():.4f} ± {variability_df['peak_height'].std():.4f}\")\n",
    "\n",
    "# # Save results to CSV for further analysis\n",
    "# output_csv_path = TESTING_RESULTS_DIR / \"aglaonema_variability_analysis.csv\"\n",
    "# variability_df.to_csv(output_csv_path, index=False)\n",
    "# print(f\"\\nResults saved to: {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2243065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variability time series plots\n",
    "print(\"Creating variability time series plots...\")\n",
    "plot_lad_variability_time_series(variability_df, FIGURE_OUTPUT_DIR)\n",
    "\n",
    "# Create distribution evolution heatmap\n",
    "print(\"Creating LAD evolution heatmap...\")\n",
    "plot_distribution_evolution_heatmap(aglaonema_results, FIGURE_OUTPUT_DIR)\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n=== LAD Variability Summary ===\")\n",
    "print(f\"Standard deviation trend: {variability_df['std_dev'].iloc[-1000:].mean():.2f} → {variability_df['std_dev'].iloc[-100:].mean():.2f}\")\n",
    "print(f\"Entropy trend: {variability_df['entropy'].iloc[-1000:].mean():.3f} → {variability_df['entropy'].iloc[-100:].mean():.3f}\")\n",
    "print(f\"CI width trend: {variability_df['ci_width'].iloc[-1000:].mean():.1f}° → {variability_df['ci_width'].iloc[-100:].mean():.1f}°\")\n",
    "\n",
    "# Save variability data for further analysis\n",
    "# variability_output_path = TESTING_RESULTS_DIR / \"aglaonema_variability_analysis.csv\"\n",
    "# variability_df.to_csv(variability_output_path, index=False)\n",
    "# print(f\"Variability data saved to: {variability_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anglecam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
